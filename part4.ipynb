{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4751d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import List, Literal\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ef3138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using CUDA')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e2af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TORCH_GENERATOR_SEED = 2147483647\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "g = torch.Generator(device=device).manual_seed(TORCH_GENERATOR_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917eec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51d88e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b1f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(str().join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d112dc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bbb8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size, device):\n",
    "    X_data, Y_data = [], []\n",
    "    for word in words:\n",
    "        context = [0 for _ in range(block_size)]\n",
    "        for ch in word + '.':\n",
    "            ix = stoi[ch]\n",
    "\n",
    "            X_data.append(context)\n",
    "            Y_data.append(ix)\n",
    "\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X_data, device=device)\n",
    "    Y = torch.tensor(Y_data, device=device)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f07c9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr.shape=torch.Size([182625, 3]), Y_tr.shape=torch.Size([182625])\n",
      "X_dev.shape=torch.Size([22655, 3]), Y_dev.shape=torch.Size([22655])\n",
      "X_te.shape=torch.Size([22866, 3]), Y_te.shape=torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "X_tr, Y_tr = build_dataset(words[:n1], block_size, device)\n",
    "X_dev, Y_dev = build_dataset(words[n1:n2], block_size, device)\n",
    "X_te, Y_te = build_dataset(words[n2:], block_size, device)\n",
    "\n",
    "print(f'{X_tr.shape=}, {Y_tr.shape=}\\n{X_dev.shape=}, {Y_dev.shape=}\\n{X_te.shape=}, {Y_te.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953c6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b5c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g, device=device)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g, device=device) * (5.0/3.0) / (math.sqrt(n_embd * block_size))\n",
    "b1 = torch.randn(n_hidden,                        generator=g, device=device) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g, device=device) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g, device=device) * 0.1\n",
    "\n",
    "bngain = torch.randn((1, n_hidden), device=device) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden), device=device) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f'Number of parameters: {sum(p.nelement() for p in parameters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5b4718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "ix = torch.randint(0, X_tr.shape[0], (batch_size,), generator=g, device=device)\n",
    "Xb, Yb = X_tr[ix], Y_tr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "382b4b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3605, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "hprebn = embcat @ W1 + b1\n",
    "\n",
    "bnmeani = 1.0 / batch_size * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1 / (batch_size-1) * bndiff2.sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes               # Subtract by max for numerical safety\n",
    "\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv                 # Multiplying by inverse instead of dividing\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0b965b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(batch_size),Yb].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef776fa",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `logprobs`.\n",
    "\n",
    "The `logprobs` tensor has dimension $32 \\times 27$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338d4057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532848f",
   "metadata": {},
   "source": [
    "Let $m = 32$ be the batch size, $n = 27$ be the vocabulary size, and $\\mathbf{LP}$ be the `logprobs` tensor.\n",
    "\n",
    "$$\n",
    "\\mathbf{LP} = \\begin{bmatrix}\n",
    "lp_{1 \\, 1} & lp_{1 \\, 2} & \\dots & lp_{1 \\, n}\\\\\n",
    "lp_{2 \\, 1} & lp_{2 \\, 2} & \\dots & lp_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "lp_{m \\, 1} & lp_{m \\, 2} & \\dots & lp_{m \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let $\\mathscr{L}$ be the loss. We want to find $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}}$.\n",
    "\n",
    "We calculate loss as\n",
    "\n",
    "```python\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "```\n",
    "\n",
    "So let's first find an expression for `-logprobs[range(batch_size), Yb]`. We call this tensor $\\mathbf{LP_b}$. We index the rows with `range(batch_size)`, which means we get all rows $0, 1, \\dots, m$. And for each row we index that row with `Yb`. This means the size of `Yb` must be the batch size. Indeed it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18367560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(range(batch_size))=32, Yb.shape=torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(range(batch_size))=}, {Yb.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066238d",
   "metadata": {},
   "source": [
    "The vector $\\mathbf{LP}_b$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\mathbf{LP}_b = \\begin{bmatrix}\n",
    "    \\mathbf{LP}_{1 \\; {\\mathbf{y}_b}_1}\\\\\n",
    "    \\mathbf{LP}_{2 \\; {\\mathbf{y}_b}_2}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\mathbf{LP}_{m \\; {\\mathbf{y}_b}_m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the loss is the negative mean of the elements in this vector.\n",
    "\n",
    "$$\n",
    "\\mathscr{L} = - \\frac{{\\mathbf{y}_b}_1 + {\\mathbf{y}_b}_2 + \\dots + {\\mathbf{y}_b}_m}{m}\n",
    "$$\n",
    "\n",
    "For the elements of $\\mathbf{LP}$ that are part of $\\mathbf{LP}_b$, the loss $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}}$ is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}} &= \\frac{\\partial}{\\partial \\mathbf{LP}_{i j}} \\left( - \\frac{{\\mathbf{y}_b}_1 + {\\mathbf{y}_b}_2 + \\dots + {\\mathbf{y}_b}_m}{m} \\right)\\\\\n",
    "    &= 0 + 0 + \\dots + \\frac{\\partial}{\\partial \\mathbf{LP}_{i j}} \\left( - \\frac{\\mathbf{LP}_{i \\, j}}{m} \\right) + \\dots + 0 + 0\\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}} &= -\\frac{1}{m}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the elements of $\\mathbf{LP}$ that are *not* part of $\\mathbf{LP}_b$, the loss $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}}$ is $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175ad230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(batch_size), Yb] = -1 / batch_size\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5cca9",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `probs`.\n",
    "\n",
    "We want to find $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}}$, where $P$ is the tensor `probs`.\n",
    "\n",
    "Note that `logprobs` is defined as\n",
    "\n",
    "```python\n",
    "logprobs = probs.log()\n",
    "```\n",
    "\n",
    "So, instead of calculating $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}}$ directly, we can use the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}} \\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}}\n",
    "$$\n",
    "\n",
    "We already know $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}}$, so we just need to calculate $\\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}}$.\n",
    "\n",
    "If $\\mathbf{LP}$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{LP} = \\begin{bmatrix}\n",
    "lp_{1 \\, 1} & lp_{1 \\, 2} & \\dots & lp_{1 \\, n}\\\\\n",
    "lp_{2 \\, 1} & lp_{2 \\, 2} & \\dots & lp_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "lp_{m \\, 1} & lp_{m \\, 2} & \\dots & lp_{m \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $\\mathbf{P}$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\begin{bmatrix}\n",
    "p_{1 \\, 1} & p_{1 \\, 2} & \\dots & p_{1 \\, n}\\\\\n",
    "p_{2 \\, 1} & p_{2 \\, 2} & \\dots & p_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "p_{m \\, 1} & p_{m \\, 2} & \\dots & p_{m \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then $\\mathbf{LP}$ can also be defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{LP} = \\log(\\mathbf{P}) = \\log\\left(\\begin{bmatrix}\n",
    "p_{1 \\, 1} & p_{1 \\, 2} & \\dots & p_{1 \\, n}\\\\\n",
    "p_{2 \\, 1} & p_{2 \\, 2} & \\dots & p_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "p_{m \\, 1} & p_{m \\, 2} & \\dots & p_{m \\, n}\\\\\n",
    "\\end{bmatrix}\\right) = \\begin{bmatrix}\n",
    "\\log(p_{1 \\, 1}) & \\log(p_{1 \\, 2}) & \\dots & \\log(p_{1 \\, n})\\\\\n",
    "\\log(p_{2 \\, 1}) & \\log(p_{2 \\, 2}) & \\dots & \\log(p_{2 \\, n})\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "\\log(p_{m \\, 1}) & \\log(p_{m \\, 2}) & \\dots & \\log(p_{m \\, n})\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So we know that $\\mathbf{LP}_{i \\, j} = \\log(\\mathbf{P}_{i \\, j})$. This means\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{LP}_{i\\,j}}{\\partial \\mathbf{P}_{i\\,j}} = \\frac{\\partial}{\\partial \\mathbf{P}_{i\\,j}} \\left( \\log(\\mathbf{P}_{i \\, j}) \\right) = \\frac{1}{\\mathbf{P}_{i \\, j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}} = \\begin{bmatrix}\n",
    "\\frac{1}{p_{1 \\, 1}} & \\frac{1}{p_{1 \\, 2}} & \\dots & \\frac{1}{p_{1 \\, n}}\\\\\n",
    "\\frac{1}{p_{2 \\, 1}} & \\frac{1}{p_{2 \\, 2}} & \\dots & \\frac{1}{p_{2 \\, n}}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "\\frac{1}{p_{m \\, 1}} & \\frac{1}{p_{m \\, 2}} & \\dots & \\frac{1}{p_{m \\, n}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we can do element-wise multiplication between $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}}$ and $\\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}}$ to get $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dd8de82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dprobs = dlogprobs * probs.pow(-1)\n",
    "cmp('probs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19bebd",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `counts_sum_inv`\n",
    "\n",
    "Note that `probs` is defined as\n",
    "\n",
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "But the sizes of `counts` and `counts_sum_inv` are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b206c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts.shape=torch.Size([32, 27]), counts_sum_inv.shape=torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f'{counts.shape=}, {counts_sum_inv.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010b359",
   "metadata": {},
   "source": [
    "This means PyTorch broadcasts the multiplication like so:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} \\circ \\mathbf{CSI} = \\begin{bmatrix}\n",
    "c_{1 \\: 1} & c_{1 \\: 2} & \\dots & c_{1 \\: 27}\\\\\n",
    "c_{2 \\: 1} & c_{2 \\: 2} & \\dots & c_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "c_{32 \\: 1} & c_{32 \\: 2} & \\dots & c_{32 \\: 27}\\\\\n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "csi_{1}\\\\\n",
    "csi_{2}\\\\\n",
    "\\vdots\\\\\n",
    "csi_{32}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "c_{1 \\: 1} & c_{1 \\: 2} & \\dots & c_{1 \\: 27}\\\\\n",
    "c_{2 \\: 1} & c_{2 \\: 2} & \\dots & c_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "c_{32 \\: 1} & c_{32 \\: 2} & \\dots & c_{32 \\: 27}\\\\\n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "csi_{1} & csi_{1} & \\dots & csi_{1}\\\\\n",
    "csi_{2} & csi_{2} & \\dots & csi_{2}\\\\\n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\\n",
    "csi_{32} & csi_{32} & \\dots & csi_{32}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "c_{1 \\: 1} \\: csi_{1} & c_{1 \\: 2} \\: csi_{1} & \\dots & c_{1 \\: 27} \\: csi_{1}\\\\\n",
    "c_{2 \\: 1} \\: csi_{2} & c_{2 \\: 2} \\: csi_{2} & \\dots & c_{2 \\: 27} \\: csi_{2}\\\\\n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\\n",
    "c_{32 \\: 1} \\: csi_{32} & c_{32 \\: 2} \\: csi_{32} & \\dots & c_{32 \\: 27} \\: csi_{32}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First let's call this broadcasted tensor $\\mathbf{CSI}'$ and calculate its partial derivative.\n",
    "\n",
    "$$\n",
    "\\mathbf{CSI}' = \\begin{bmatrix}\n",
    "csi_{1} & csi_{1} & \\dots & csi_{1}\\\\\n",
    "csi_{2} & csi_{2} & \\dots & csi_{2}\\\\\n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\\n",
    "csi_{32} & csi_{32} & \\dots & csi_{32}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CSI}'} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}} \\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{CSI}'}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{P} = \\mathbf{C} \\circ \\mathbf{CSI}'$, $\\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{CSI}'}$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{CSI}'} = \\frac{\\partial}{\\partial \\mathbf{CSI}'} \\left( \\mathbf{C} \\circ \\mathbf{CSI}' \\right) = \\mathbf{C}\n",
    "$$\n",
    "\n",
    "Since the gradient of the vector $\\mathbf{CSI}$ is used multiple times (each column in the broadcast), we must sum each use of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92790ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cea05c",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `counts_sum`\n",
    "\n",
    "Note that `counts_sum_inv` is defined as\n",
    "\n",
    "```python\n",
    "counts_sum_inv = counts_sum**-1\n",
    "```\n",
    "\n",
    "Denoting `counts_sum_inv` as $\\mathbf{CSI}$ and `counts_sum` as $\\mathbf{CS}$\n",
    "\n",
    "$$\n",
    "\\mathbf{CSI} = \\mathbf{CS}^{-1}\n",
    "$$\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CS}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CSI}} \\frac{\\partial \\mathbf{CSI}}{\\partial \\mathbf{CS}}\n",
    "$$\n",
    "\n",
    "All we need to compute is $\\frac{\\partial \\mathbf{CSI}}{\\partial \\mathbf{CS}}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{CSI}}{\\partial \\mathbf{CS}} = \\frac{\\partial}{\\partial \\mathbf{CS}} \\left( \\mathbf{CS}^{-1} \\right) = - \\mathbf{CS}^{-2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13c5272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum = dcounts_sum_inv * - counts_sum.pow(-2)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb685980",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `counts`\n",
    "\n",
    "Note that `probs` is defined as\n",
    "\n",
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "and `counts_sum` is defined as\n",
    "\n",
    "```python\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "```\n",
    "\n",
    "Representing `probs` as $\\mathbf{P}$, `counts` as $\\mathbf{C}$, `counts_sum` as $\\mathbf{CS}$, and `counts_sum_inv` as $\\mathbf{CSI}$, we get two equations involving $\\mathbf{C}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{P} = \\mathbf{C} * \\mathbf{CSI}\\\\\n",
    "    \\mathbf{CS} = \\sum_{i} \\mathbf{C}_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To calculate $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}}$, we must calculate the gradient for both of the above equations and add their contributions to the gradient of $\\mathbf{C}$.\n",
    "\n",
    "1. Remember, the sizes of `counts` and `counts_sum_inv` are different, causing `counts_sum_inv` to be broadcasted.\n",
    "\n",
    "    Using the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}} \\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{C}}\n",
    "    $$\n",
    "\n",
    "    All we need to compute is $\\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{C}}$, denoting the broadcasted tensor of $\\mathbf{CSI}$ as $\\mathbf{CSI}'$.\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{C}} = \\frac{\\partial}{\\partial \\mathbf{C}} \\left( \\mathbf{C} \\circ \\mathbf{CSI}' \\right) = \\mathbf{CSI}'\n",
    "    $$\n",
    "\n",
    "2. Defining $\\mathbf{C}$ as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{C} = \\begin{bmatrix}\n",
    "    c_{1 \\: 1} & c_{1 \\: 2} & \\dots & c_{1 \\: 27}\\\\\n",
    "    c_{2 \\: 1} & c_{2 \\: 2} & \\dots & c_{2 \\: 27}\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    c_{32 \\: 1} & c_{32 \\: 2} & \\dots & c_{32 \\: 27}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    we know that $\\mathbf{CS}$ is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{CS} = \\begin{bmatrix}\n",
    "    c_{1 \\: 1} + c_{1 \\: 2} + \\dots + c_{1 \\: 27}\\\\\n",
    "    c_{2 \\: 1} + c_{2 \\: 2} + \\dots + c_{2 \\: 27}\\\\\n",
    "    \\vdots\\\\\n",
    "    c_{32 \\: 1} + c_{32 \\: 2} + \\dots + c_{32 \\: 27}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    Using the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CS}} \\frac{\\partial \\mathbf{CS}}{\\partial \\mathbf{C}}\n",
    "    $$\n",
    "\n",
    "    We only need to calculate $\\frac{\\partial \\mathbf{CS}}{\\partial \\mathbf{C}}$.\n",
    "\n",
    "    Since each element $c_{i j}$ of $\\mathbf{C}$ participates in a sum in some element of $\\mathbf{CS}$, the partial derivative $\\frac{\\partial \\mathbf{CS}_i}{\\partial \\mathbf{C}_{i j}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathbf{CS}_i}{\\partial \\mathbf{C}_{i j}} = \\frac{\\partial}{\\partial \\mathbf{C}_{i j}} \\left( c_{i \\, 1} + c_{i \\, 2} + \\dots + c_{i j} + \\dots c_{i \\, 27} \\right) = 1\n",
    "    $$\n",
    "\n",
    "    So, the partial derivative $\\frac{\\partial \\mathbf{CS}}{\\partial \\mathbf{C}}$ is a matrix the shape of $\\mathbf{C}$ with all elements being $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2f68d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts = (counts_sum_inv * dprobs) + (torch.ones_like(counts) * dcounts_sum)\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a3bb1",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `norm_logits`\n",
    "\n",
    "Note that `counts` is defined as\n",
    "\n",
    "```python\n",
    "counts = norm_logits.exp()\n",
    "```\n",
    "\n",
    "Representing `counts` as $\\mathbf{C}$ and `norm_logits` as $\\mathbf{NL}$, we have:\n",
    "\n",
    "$$\n",
    "\\mathbf{NL} = \\begin{bmatrix}\n",
    "nl_{1 \\: 1} & nl_{1 \\: 2} & \\dots & nl_{1 \\: 27}\\\\\n",
    "nl_{2 \\: 1} & nl_{2 \\: 2} & \\dots & nl_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "nl_{32 \\: 1} & nl_{32 \\: 2} & \\dots & nl_{32 \\: 27}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This way, $\\mathbf{C}$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\exp(\\mathbf{NL}) = \\begin{bmatrix}\n",
    "e^{nl_{1 \\: 1}} & e^{nl_{1 \\: 2}} & \\dots & e^{nl_{1 \\: 27}}\\\\\n",
    "e^{nl_{2 \\: 1}} & e^{nl_{2 \\: 2}} & \\dots & e^{nl_{2 \\: 27}}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "e^{nl_{32 \\: 1}} & e^{nl_{32 \\: 2}} & \\dots & e^{nl_{32 \\: 27}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{NL}}\n",
    "$$\n",
    "\n",
    "For each partial derivative $\\frac{\\partial \\mathbf{C}_{ij}}{\\partial \\mathbf{NL}_{ij}}$, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{C}_{ij}}{\\partial \\mathbf{NL}_{ij}} = \\frac{\\partial}{\\partial \\mathbf{NL}_{ij}}\\left( e^{nl_{ij}} \\right) = e^{nl_{ij}}\n",
    "$$\n",
    "\n",
    "Therefore, $\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{NL}} = \\mathbf{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5632a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dnorm_logits = counts * dcounts\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d853c25",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `logits_maxes`\n",
    "\n",
    "Note that `norm_logits` is defined as\n",
    "\n",
    "```python\n",
    "norm_logits = logits - logits_maxes\n",
    "```\n",
    "\n",
    "Since `logits` and `logits_maxes` are not the same shape, the operation will be broadcasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6421b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([32, 27]), logit_maxes.shape=torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f'{logits.shape=}, {logit_maxes.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f2a1a",
   "metadata": {},
   "source": [
    "This means the vector `logit_maxes` will be repeated $27$ times. So `norm_logits` is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{NL} = \\begin{bmatrix}\n",
    "l_{1 \\: 1} - lm_1 & l_{1 \\: 2} - lm_1 & \\dots & l_{1 \\: 27} - lm_1\\\\\n",
    "l_{2 \\: 1} - lm_2 & l_{2 \\: 2} - lm_2 & \\dots & l_{2 \\: 27} - lm_2\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "l_{32 \\: 1} - lm_{32} & l_{32 \\: 2} - lm_{32} & \\dots & l_{32 \\: 27} - lm_{32}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The gradient $\\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}}$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}} = \\begin{bmatrix}\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "\\vdots\\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Again using the chain rule, $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LM}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}} \\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}}$. Again, due to broadcasting, $\\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}}$ is repeated. Since all its entries are $-1$, we can sum across $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}}$ and negate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "558f2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogit_maxes = - dnorm_logits.sum(1, keepdim=True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4554c",
   "metadata": {},
   "source": [
    "Note that the logit normalization is performed to prevent floating point errors when values are too large. This means it should not have an effect on the loss, which means the gradient of `dlogit_maxes` should be approximately $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "891db07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.3132e-10],\n",
       "        [-5.5879e-09],\n",
       "        [-1.8626e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [-9.3132e-10],\n",
       "        [-9.3132e-10],\n",
       "        [ 2.7940e-09],\n",
       "        [-0.0000e+00],\n",
       "        [-9.3132e-10],\n",
       "        [-3.7253e-09],\n",
       "        [ 9.3132e-10],\n",
       "        [ 9.3132e-10],\n",
       "        [-0.0000e+00],\n",
       "        [-9.3132e-10],\n",
       "        [ 1.8626e-09],\n",
       "        [-0.0000e+00],\n",
       "        [ 3.7253e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [-4.6566e-09],\n",
       "        [-5.5879e-09],\n",
       "        [-3.7253e-09],\n",
       "        [ 9.3132e-10],\n",
       "        [ 3.7253e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [-9.3132e-10],\n",
       "        [ 4.6566e-09],\n",
       "        [-3.7253e-09],\n",
       "        [ 9.3132e-10],\n",
       "        [-3.7253e-09],\n",
       "        [-0.0000e+00],\n",
       "        [-0.0000e+00],\n",
       "        [-9.3132e-10]], device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37c813",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `logits`\n",
    "\n",
    "The tensor `logits` is used twice, so its gradient is a sum of the two separate gradients.\n",
    "\n",
    "```python\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "```\n",
    "\n",
    "1. The `logits` tensor $\\mathbf{L}$ is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{L} = \\begin{bmatrix}\n",
    "    l_{1 \\: 1} & l_{1 \\: 2} & \\dots & l_{1 \\: 27}\\\\\n",
    "    l_{2 \\: 1} & l_{2 \\: 2} & \\dots & l_{2 \\: 27}\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    l_{32 \\: 1} & l_{32 \\: 2} & \\dots & l_{32 \\: 27}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The `logit_maxes` tensor is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{LM} = \\begin{bmatrix}\n",
    "    lm_1\\\\\n",
    "    lm_2\\\\\n",
    "    \\vdots\\\\\n",
    "    lm_{32}\\\\\n",
    "    \\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\max(l_{1 \\: 1}, l_{1 \\: 2}, \\dots, l_{1 \\: 27})\\\\\n",
    "    \\max(l_{2 \\: 1}, l_{2 \\: 2}, \\dots, l_{2 \\: 27})\\\\\n",
    "    \\vdots\\\\\n",
    "    \\max(l_{32 \\: 1}, l_{32 \\: 2}, \\dots, l_{32 \\: 27})\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The partial derivative $\\frac{\\partial \\mathbf{LM}_{i}}{\\partial \\mathbf{L}_{i j}}$ is $1$ if $l_{i j}$ is the maximum of the row $l_{i \\, 1}, l_{i \\, 2}, \\dots l_{i \\, j}, \\dots, l_{i \\, 27}$, and is $0$ otherwise. From here we can use the chain rule $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LM}} \\frac{\\partial \\mathbf{LM}}{\\partial \\mathbf{L}}$\n",
    "\n",
    "2. The `norm_logits` tensor $\\mathbf{NL}$ is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{NL} = \\begin{bmatrix}\n",
    "    l_{1 \\: 1} - lm_1 & l_{1 \\: 2} - lm_1 & \\dots & l_{1 \\: 27} - lm_1\\\\\n",
    "    l_{2 \\: 1} - lm_2 & l_{2 \\: 2} - lm_2 & \\dots & l_{2 \\: 27} - lm_2\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    l_{32 \\: 1} - lm_{32} & l_{32 \\: 2} - lm_{32} & \\dots & l_{32 \\: 27} - lm_{32}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The partial derivative $\\frac{\\partial \\mathbf{NL}_{i j}}{\\partial \\mathbf{L}_{i j}}$ is $1$ because all $l_{i j}$ participate in the $\\mathbf{NL}$ matrix with coefficient $1$. This means\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{L}} = \\begin{bmatrix}\n",
    "    1 & 1 & \\dots & 1\\\\\n",
    "    1 & 1 & \\dots & 1\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    1 & 1 & \\dots & 1\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    From here we can use the chain rule $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}} \\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{L}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "986d420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogits = (F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes) + (dnorm_logits)\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd1867",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `h`, `W2`, and `b2`\n",
    "\n",
    "The `logits` tensor is defined as\n",
    "\n",
    "```\n",
    "logits = h @ W2 + b2\n",
    "```\n",
    "\n",
    "Defining the $\\mathbf{H}$, $\\mathbf{W}_2$, and $\\mathbf{b}_2$ tensors as\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\begin{bmatrix}\n",
    "h_{1 \\: 1} & h_{1 \\: 2} & \\dots & h_{1 \\: 64}\\\\\n",
    "h_{2 \\: 1} & h_{2 \\: 2} & \\dots & h_{2 \\: 64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "h_{32 \\: 1} & h_{32 \\: 2} & \\dots & h_{32 \\: 64}\\\\\n",
    "\\end{bmatrix} \\quad\\quad \\mathbf{W}_2 = \\begin{bmatrix}\n",
    "w_{1 \\: 1} & w_{1 \\: 2} & \\dots & w_{1 \\: 27}\\\\\n",
    "w_{2 \\: 1} & w_{2 \\: 2} & \\dots & w_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "w_{64 \\: 1} & w_{64 \\: 2} & \\dots & w_{64 \\: 27}\\\\\n",
    "\\end{bmatrix} \\quad\\quad \\mathbf{b}_2 = \\begin{bmatrix}\n",
    "b_{1} & b_{2} & \\dots & b_{27}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The product $\\mathbf{H} \\mathbf{W}_2$ is performed as\n",
    "\n",
    "$$\n",
    "\\mathbf{H} \\mathbf{W}_2 = \\begin{bmatrix}\n",
    "h_{1 \\: 1} \\, w_{1 \\: 1} + h_{1 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 1} & h_{1 \\: 1} \\, w_{1 \\: 2} + h_{1 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 2} & \\dots & h_{1 \\: 1} \\, w_{1 \\: 27} + h_{1 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 27}\\\\\n",
    "h_{2 \\: 1} \\, w_{1 \\: 1} + h_{2 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 1} & h_{2 \\: 1} \\, w_{1 \\: 2} + h_{2 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 2} & \\dots & h_{2 \\: 1} \\, w_{1 \\: 27} + h_{2 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "h_{32 \\: 1} \\, w_{1 \\: 1} + h_{32 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 1} & h_{32 \\: 1} \\, w_{1 \\: 2} + h_{32 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 2} & \\dots & h_{32 \\: 1} \\, w_{1 \\: 27} + h_{32 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 27}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the full expression $\\mathbf{L} = \\mathbf{H} \\mathbf{W}_2 + \\mathbf{b}_2$, which causes $\\mathbf{b}_2$ to be broadcasted, is\n",
    "\n",
    "$$\n",
    "\\mathbf{L} = \\mathbf{H} \\mathbf{W}_2 + \\mathbf{b}_2 = \\begin{bmatrix}\n",
    "h_{1 \\: 1} \\, w_{1 \\: 1} + h_{1 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 1} + b_1 & h_{1 \\: 1} \\, w_{1 \\: 2} + h_{1 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 2} + b_2 & \\dots & h_{1 \\: 1} \\, w_{1 \\: 27} + h_{1 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 27} + b_{27}\\\\\n",
    "h_{2 \\: 1} \\, w_{1 \\: 1} + h_{2 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 1} + b_1 & h_{2 \\: 1} \\, w_{1 \\: 2} + h_{2 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 2} + b_2 & \\dots & h_{2 \\: 1} \\, w_{1 \\: 27} + h_{2 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 27} + b_{27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "h_{32 \\: 1} \\, w_{1 \\: 1} + h_{32 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 1} + b_{1} & h_{32 \\: 1} \\, w_{1 \\: 2} + h_{32 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 2} + b_{2} & \\dots & h_{32 \\: 1} \\, w_{1 \\: 27} + h_{32 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 27} + b_{27}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For $\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}}$, the partial derivative for entry $i,j$, $\\frac{\\partial \\mathbf{L}_{i j}}{\\partial \\mathbf{H}_{i j}}$, is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}_{i j}}{\\partial \\mathbf{H}_{i j}} = \\frac{\\partial}{\\partial h_{i j}} \\left( h_{i \\: 1} \\, w_{1 \\: i} + h_{i \\: 2} \\, w_{2 \\: i} + \\dots + h_{i \\: j} \\, w_{j \\: i} + \\dots + h_{i \\: 64} \\, w_{64 \\: i} + b_j \\right) = w_{j \\: i}\n",
    "$$\n",
    "\n",
    "which means the partial derivative $\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}}$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}} = {\\mathbf{W}_2}^\\mathrm{T}\n",
    "$$\n",
    "\n",
    "By similar logic, the partial derivative $\\frac{\\partial \\mathbf{L}}{\\partial {\\mathbf{W}_2}}$, is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial {\\mathbf{W}_2}}\n",
    "$$\n",
    "\n",
    "Lastly, for $\\mathbf{b}_2$, the partial derivative $\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{b}_2}$ for entry $i,j$, $\\frac{\\partial \\mathbf{L}_{i j}}{\\partial {\\mathbf{b}_2}_{i j}}$, is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}_{i j}}{\\partial {\\mathbf{b}_2}_{i j}} = \\frac{\\partial}{\\partial {b}_{j}} \\left( h_{i \\: 1} \\, w_{1 \\: i} + h_{i \\: 2} \\, w_{2 \\: i} + \\dots + h_{i \\: j} \\, w_{j \\: i} + \\dots + h_{i \\: 64} \\, w_{64 \\: i} + b_j \\right) = 1\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{b}_2$ is broadcasted, there are $64$ contributions to the gradient $\\frac{\\partial \\mathbf{L}}{\\partial {\\mathbf{b}_2}}$ (one for each row).\n",
    "\n",
    "From here, we can use the chain rule to calculate the derivatives of $\\mathbf{L}$ with respect to $\\mathbf{H}$, $\\mathbf{W}_2$, and $\\mathbf{b}_2$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{W}_2} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{W}_2}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{b}_2} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{b}_2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f93c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowing the shapes of the tensors involved in the matrix multiplication can help you figure out the correct way to multiply the gradients.\n",
      "logits.shape=torch.Size([32, 27]), W2.shape=torch.Size([64, 27]), h.shape=torch.Size([32, 64]), b2.shape=torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "print('Knowing the shapes of the tensors involved in the matrix multiplication can help you figure out the correct way to multiply the gradients.')\n",
    "print(f'{logits.shape=}, {W2.shape=}, {h.shape=}, {b2.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7433241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0, keepdim=True)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf08476",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `hpreact`\n",
    "\n",
    "The tensor `h` is defined as\n",
    "\n",
    "```python\n",
    "h = torch.tanh(hpreact)\n",
    "```\n",
    "\n",
    "Note the derivative of $\\mathrm{tanh}$\n",
    "\n",
    "$$\n",
    "y = \\mathrm{tanh}(x) \\Rightarrow \\frac{dy}{dx} = 1 - y^2\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{H} = \\mathrm{tanh}(\\mathbf{H}_{pre \\, activation})$, we know that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{H}}{\\partial \\mathbf{H}_{pre \\,activation}} = 1 - \\mathbf{H}^2\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}} \\frac{\\partial \\mathbf{H}}{\\partial \\mathbf{H}_{pre \\, activation}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e2dea48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n"
     ]
    }
   ],
   "source": [
    "dhpreact = dh * (1.0 - h.pow(2))\n",
    "cmp('hpreact', dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6936812",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bngain`, `bnraw`, and `bnbias`\n",
    "\n",
    "The tensor `hpreact` is defined as\n",
    "\n",
    "```python\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```\n",
    "\n",
    "Note the shapes of each tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8716e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact.shape=torch.Size([32, 64]), bngain.shape=torch.Size([1, 64]), bnraw.shape=torch.Size([32, 64]), bnbias.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{hpreact.shape=}, {bngain.shape=}, {bnraw.shape=}, {bnbias.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fcb1d",
   "metadata": {},
   "source": [
    "Defining the ${\\mathbf{b}_{n}}_{gain}$, ${\\mathbf{B}_{n}}_{raw}$, and ${\\mathbf{b}_{n}}_{bias}$ tensors as\n",
    "\n",
    "$$\n",
    "{\\mathbf{b}_{n}}_{gain} = \\begin{bmatrix}\n",
    "{{b_n}_{gain}}_{1} & {{b_n}_{gain}}_{2} & \\dots & {{b_n}_{gain}}_{64}\n",
    "\\end{bmatrix} \\quad\\quad {\\mathbf{B}_{n}}_{raw} = \\begin{bmatrix}\n",
    "{{b_n}_{raw}}_{1 \\: 1} & {{b_n}_{raw}}_{1 \\: 2} & \\dots & {{b_n}_{raw}}_{1 \\: 64}\\\\\n",
    "{{b_n}_{raw}}_{2 \\: 1} & {{b_n}_{raw}}_{2 \\: 2} & \\dots & {{b_n}_{raw}}_{2 \\: 64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "{{b_n}_{raw}}_{32 \\: 1} & {{b_n}_{raw}}_{32 \\: 2} & \\dots & {{b_n}_{raw}}_{32 \\: 64}\\\\\n",
    "\\end{bmatrix} \\quad\\quad {\\mathbf{b}_{n}}_{bias} = \\begin{bmatrix}\n",
    "{{b_n}_{bias}}_{1} & {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{bias}}_{64}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The full operation $\\mathbf{H}_{pre \\, activation} = {\\mathbf{b}_{n}}_{gain} \\circ {\\mathbf{B}_{n}}_{raw} + {\\mathbf{b}_{n}}_{bias}$, including broadcasting, is\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_{pre \\, activation} = {\\mathbf{b}_{n}}_{gain} \\circ {\\mathbf{B}_{n}}_{raw} + {\\mathbf{b}_{n}}_{bias} = \\begin{bmatrix}\n",
    "{{b_n}_{gain}}_{1} \\: {{b_n}_{raw}}_{1 \\: 1} + {{b_n}_{bias}}_{1} & {{b_n}_{gain}}_{2} \\: {{b_n}_{raw}}_{1 \\: 2} + {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{gain}}_{64} \\: {{b_n}_{raw}}_{1 \\: 64} + {{b_n}_{bias}}_{64}\\\\\n",
    "{{b_n}_{gain}}_{1} \\: {{b_n}_{raw}}_{2 \\: 1} + {{b_n}_{bias}}_{1} & {{b_n}_{gain}}_{2} \\: {{b_n}_{raw}}_{2 \\: 2} + {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{gain}}_{64} \\: {{b_n}_{raw}}_{2 \\: 64} + {{b_n}_{bias}}_{64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "{{b_n}_{gain}}_{1} \\: {{b_n}_{raw}}_{32 \\: 1} + {{b_n}_{bias}}_{1} & {{b_n}_{gain}}_{2} \\: {{b_n}_{raw}}_{32 \\: 2} + {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{gain}}_{64} \\: {{b_n}_{raw}}_{32 \\: 64} + {{b_n}_{bias}}_{64}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculating the derivative $\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{gain}}}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}_{i j}}{\\partial {{\\mathbf{b}_{n}}_{gain}}_{i j}} = \\frac{\\partial}{\\partial {{{b}_{n}}_{gain}}_{j}} \\left( {{b_n}_{gain}}_{j} \\: {{b_n}_{raw}}_{i \\: j} + {{b_n}_{bias}}_{j} \\right) = {{b_n}_{raw}}_{i \\: j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{gain}}} = {\\mathbf{B}_{n}}_{raw}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Calculating the derivative $\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{B}_{n}}_{raw}}}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}_{i j}}{\\partial {{\\mathbf{B}_{n}}_{raw}}_{i j}} = \\frac{\\partial}{\\partial {{{b}_{n}}_{gain}}_{j}} \\left( {{b_n}_{gain}}_{j} \\: {{b_n}_{raw}}_{i \\: j} + {{b_n}_{bias}}_{j} \\right) = {{b_n}_{gain}}_{j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{B}_{n}}_{raw}}} = {\\mathbf{b}_{n}}_{gain}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Calculating the derivative $\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{bias}}}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}_{i j}}{\\partial {{\\mathbf{b}_{n}}_{bias}}_{i j}} = \\frac{\\partial}{\\partial {{{b}_{n}}_{gain}}_{j}} \\left( {{b_n}_{gain}}_{j} \\: {{b_n}_{raw}}_{i \\: j} + {{b_n}_{bias}}_{j} \\right) = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{bias}}} = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{b}_{n}}_{gain}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} \\frac{\\partial \\mathbf{H}_{pre \\, activation}}{\\partial {\\mathbf{b}_{n}}_{gain}}\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_{n}}_{raw}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} \\frac{\\partial \\mathbf{H}_{pre \\, activation}}{\\partial {\\mathbf{B}_{n}}_{raw}}\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{b}_{n}}_{bias}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} \\frac{\\partial \\mathbf{H}_{pre \\, activation}}{\\partial {\\mathbf{b}_{n}}_{bias}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1948637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "dbnraw = dhpreact * bngain\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnbias', dbnbias, bnbias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba1534",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `dbnvar_inv`\n",
    "\n",
    "The tensor `bnraw` is defined as\n",
    "\n",
    "```python\n",
    "bnraw = bndiff * bnvar_inv\n",
    "```\n",
    "\n",
    "Note the shapes are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85f1d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnraw.shape=torch.Size([32, 64]), bndiff.shape=torch.Size([32, 64]), bnvar_inv.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{bnraw.shape=}, {bndiff.shape=}, {bnvar_inv.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ff900",
   "metadata": {},
   "source": [
    "So `bnvar_inv` will be broadcasted.\n",
    "\n",
    "We know how to do back-propagation for element-wise multiplication: we just multiply ${\\mathbf{B}_n}_{diff}$ by $\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{raw}}$ and sum across the rows (because of broadcasting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c655d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n"
     ]
    }
   ],
   "source": [
    "dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim=True)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3bc3d",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bnvar`\n",
    "\n",
    "The tensor `bnraw` is defined as\n",
    "\n",
    "```python\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "```\n",
    "\n",
    "Taking the derivative \n",
    "\n",
    "$$\n",
    "\\frac{\\partial {{\\mathbf{B}_n}_{var}}_{inv}}{\\partial {\\mathbf{B}_n}_{var}} = \\frac{\\partial}{\\partial {\\mathbf{B}_n}_{var}} \\left( \\left( {\\mathbf{B}_n}_{var} + 1 \\times 10^{-5} \\right)^{-\\frac{1}{2}} \\right) = - \\frac{1}{2} \\left( {\\mathbf{B}_n}_{var} + 1 \\times 10^{-5} \\right)^{-\\frac{3}{2}}\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{var}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{B}_n}_{var}}_{inv}} \\frac{\\partial {{\\mathbf{B}_n}_{var}}_{inv}}{\\partial {\\mathbf{B}_n}_{var}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ef6ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "dbnvar = dbnvar_inv * (-(1.0/2.0) * (bnvar + 1e-5)**(-3.0/2.0))\n",
    "cmp('bnvar', dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78f46a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar.shape=torch.Size([1, 64]), bnvar_inv.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{bnvar.shape=}, {bnvar_inv.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d60b96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = C[Xb]\n",
    "# embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "# hprebn = embcat @ W1 + b1\n",
    "\n",
    "# bnmeani = 1.0 / batch_size * hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1 / (batch_size-1) * bndiff2.sum(0, keepdim=True)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "\n",
    "# hpreact = bngain + bnraw + bnbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "051223fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 27])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01a05a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eab79cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a916f77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts_sum.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
