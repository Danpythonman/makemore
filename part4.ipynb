{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4751d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import List, Literal\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ef3138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using CUDA')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e2af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TORCH_GENERATOR_SEED = 2147483647\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "g = torch.Generator(device=device).manual_seed(TORCH_GENERATOR_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917eec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51d88e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b1f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(str().join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d112dc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bbb8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size, device):\n",
    "    X_data, Y_data = [], []\n",
    "    for word in words:\n",
    "        context = [0 for _ in range(block_size)]\n",
    "        for ch in word + '.':\n",
    "            ix = stoi[ch]\n",
    "\n",
    "            X_data.append(context)\n",
    "            Y_data.append(ix)\n",
    "\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X_data, device=device)\n",
    "    Y = torch.tensor(Y_data, device=device)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f07c9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr.shape=torch.Size([182625, 3]), Y_tr.shape=torch.Size([182625])\n",
      "X_dev.shape=torch.Size([22655, 3]), Y_dev.shape=torch.Size([22655])\n",
      "X_te.shape=torch.Size([22866, 3]), Y_te.shape=torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "X_tr, Y_tr = build_dataset(words[:n1], block_size, device)\n",
    "X_dev, Y_dev = build_dataset(words[n1:n2], block_size, device)\n",
    "X_te, Y_te = build_dataset(words[n2:], block_size, device)\n",
    "\n",
    "print(f'{X_tr.shape=}, {Y_tr.shape=}\\n{X_dev.shape=}, {Y_dev.shape=}\\n{X_te.shape=}, {Y_te.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953c6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b5c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g, device=device)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g, device=device) * (5.0/3.0) / (math.sqrt(n_embd * block_size))\n",
    "b1 = torch.randn(n_hidden,                        generator=g, device=device) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g, device=device) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g, device=device) * 0.1\n",
    "\n",
    "bngain = torch.randn((1, n_hidden), device=device) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden), device=device) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f'Number of parameters: {sum(p.nelement() for p in parameters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5b4718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "ix = torch.randint(0, X_tr.shape[0], (batch_size,), generator=g, device=device)\n",
    "Xb, Yb = X_tr[ix], Y_tr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "382b4b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3541, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "hprebn = embcat @ W1 + b1\n",
    "\n",
    "bnmeani = 1.0 / batch_size * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1 / (batch_size-1) * bndiff2.sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes               # Subtract by max for numerical safety\n",
    "\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv                 # Multiplying by inverse instead of dividing\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0b965b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(batch_size),Yb].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef776fa",
   "metadata": {},
   "source": [
    "## Exercise 1 - Back-Propagation Manually through each Intermediate Step\n",
    "\n",
    "### Derivative of `loss` with respect to `logprobs`.\n",
    "\n",
    "The `logprobs` tensor has dimension $32 \\times 27$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338d4057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532848f",
   "metadata": {},
   "source": [
    "Let $m = 32$ be the batch size, $n = 27$ be the vocabulary size, and $\\mathbf{LP}$ be the `logprobs` tensor.\n",
    "\n",
    "$$\n",
    "\\mathbf{LP} = \\begin{bmatrix}\n",
    "lp_{1 \\, 1} & lp_{1 \\, 2} & \\dots & lp_{1 \\, n}\\\\\n",
    "lp_{2 \\, 1} & lp_{2 \\, 2} & \\dots & lp_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "lp_{m \\, 1} & lp_{m \\, 2} & \\dots & lp_{m \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let $\\mathscr{L}$ be the loss. We want to find $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}}$.\n",
    "\n",
    "We calculate loss as\n",
    "\n",
    "```python\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "```\n",
    "\n",
    "So let's first find an expression for `-logprobs[range(batch_size), Yb]`. We call this tensor $\\mathbf{LP_b}$. We index the rows with `range(batch_size)`, which means we get all rows $0, 1, \\dots, m$. And for each row we index that row with `Yb`. This means the size of `Yb` must be the batch size. Indeed it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18367560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(range(batch_size))=32, Yb.shape=torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(range(batch_size))=}, {Yb.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066238d",
   "metadata": {},
   "source": [
    "The vector $\\mathbf{LP}_b$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\mathbf{LP}_b = \\begin{bmatrix}\n",
    "    \\mathbf{LP}_{1 \\; {\\mathbf{y}_b}_1}\\\\\n",
    "    \\mathbf{LP}_{2 \\; {\\mathbf{y}_b}_2}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\mathbf{LP}_{m \\; {\\mathbf{y}_b}_m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the loss is the negative mean of the elements in this vector.\n",
    "\n",
    "$$\n",
    "\\mathscr{L} = - \\frac{{\\mathbf{y}_b}_1 + {\\mathbf{y}_b}_2 + \\dots + {\\mathbf{y}_b}_m}{m}\n",
    "$$\n",
    "\n",
    "For the elements of $\\mathbf{LP}$ that are part of $\\mathbf{LP}_b$, the loss $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}}$ is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}} &= \\frac{\\partial}{\\partial \\mathbf{LP}_{i j}} \\left( - \\frac{{\\mathbf{y}_b}_1 + {\\mathbf{y}_b}_2 + \\dots + {\\mathbf{y}_b}_m}{m} \\right)\\\\\n",
    "    &= 0 + 0 + \\dots + \\frac{\\partial}{\\partial \\mathbf{LP}_{i j}} \\left( - \\frac{\\mathbf{LP}_{i \\, j}}{m} \\right) + \\dots + 0 + 0\\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}} &= -\\frac{1}{m}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the elements of $\\mathbf{LP}$ that are *not* part of $\\mathbf{LP}_b$, the loss $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}}$ is $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175ad230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(batch_size), Yb] = -1 / batch_size\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5cca9",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `probs`.\n",
    "\n",
    "We want to find $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}}$, where $P$ is the tensor `probs`.\n",
    "\n",
    "Note that `logprobs` is defined as\n",
    "\n",
    "```python\n",
    "logprobs = probs.log()\n",
    "```\n",
    "\n",
    "So, instead of calculating $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}}$ directly, we can use the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}} \\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}}\n",
    "$$\n",
    "\n",
    "We already know $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}}$, so we just need to calculate $\\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}}$.\n",
    "\n",
    "If $\\mathbf{LP}$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{LP} = \\begin{bmatrix}\n",
    "lp_{1 \\, 1} & lp_{1 \\, 2} & \\dots & lp_{1 \\, n}\\\\\n",
    "lp_{2 \\, 1} & lp_{2 \\, 2} & \\dots & lp_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "lp_{m \\, 1} & lp_{m \\, 2} & \\dots & lp_{m \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $\\mathbf{P}$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\begin{bmatrix}\n",
    "p_{1 \\, 1} & p_{1 \\, 2} & \\dots & p_{1 \\, n}\\\\\n",
    "p_{2 \\, 1} & p_{2 \\, 2} & \\dots & p_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "p_{m \\, 1} & p_{m \\, 2} & \\dots & p_{m \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then $\\mathbf{LP}$ can also be defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{LP} = \\log(\\mathbf{P}) = \\log\\left(\\begin{bmatrix}\n",
    "p_{1 \\, 1} & p_{1 \\, 2} & \\dots & p_{1 \\, n}\\\\\n",
    "p_{2 \\, 1} & p_{2 \\, 2} & \\dots & p_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "p_{m \\, 1} & p_{m \\, 2} & \\dots & p_{m \\, n}\\\\\n",
    "\\end{bmatrix}\\right) = \\begin{bmatrix}\n",
    "\\log(p_{1 \\, 1}) & \\log(p_{1 \\, 2}) & \\dots & \\log(p_{1 \\, n})\\\\\n",
    "\\log(p_{2 \\, 1}) & \\log(p_{2 \\, 2}) & \\dots & \\log(p_{2 \\, n})\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "\\log(p_{m \\, 1}) & \\log(p_{m \\, 2}) & \\dots & \\log(p_{m \\, n})\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So we know that $\\mathbf{LP}_{i \\, j} = \\log(\\mathbf{P}_{i \\, j})$. This means\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{LP}_{i\\,j}}{\\partial \\mathbf{P}_{i\\,j}} = \\frac{\\partial}{\\partial \\mathbf{P}_{i\\,j}} \\left( \\log(\\mathbf{P}_{i \\, j}) \\right) = \\frac{1}{\\mathbf{P}_{i \\, j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}} = \\begin{bmatrix}\n",
    "\\frac{1}{p_{1 \\, 1}} & \\frac{1}{p_{1 \\, 2}} & \\dots & \\frac{1}{p_{1 \\, n}}\\\\\n",
    "\\frac{1}{p_{2 \\, 1}} & \\frac{1}{p_{2 \\, 2}} & \\dots & \\frac{1}{p_{2 \\, n}}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "\\frac{1}{p_{m \\, 1}} & \\frac{1}{p_{m \\, 2}} & \\dots & \\frac{1}{p_{m \\, n}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we can do element-wise multiplication between $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}}$ and $\\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}}$ to get $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dd8de82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dprobs = dlogprobs * probs.pow(-1)\n",
    "cmp('probs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19bebd",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `counts_sum_inv`\n",
    "\n",
    "Note that `probs` is defined as\n",
    "\n",
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "But the sizes of `counts` and `counts_sum_inv` are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b206c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts.shape=torch.Size([32, 27]), counts_sum_inv.shape=torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f'{counts.shape=}, {counts_sum_inv.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010b359",
   "metadata": {},
   "source": [
    "This means PyTorch broadcasts the multiplication like so:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} \\circ \\mathbf{CSI} = \\begin{bmatrix}\n",
    "c_{1 \\: 1} & c_{1 \\: 2} & \\dots & c_{1 \\: 27}\\\\\n",
    "c_{2 \\: 1} & c_{2 \\: 2} & \\dots & c_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "c_{32 \\: 1} & c_{32 \\: 2} & \\dots & c_{32 \\: 27}\\\\\n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "csi_{1}\\\\\n",
    "csi_{2}\\\\\n",
    "\\vdots\\\\\n",
    "csi_{32}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "c_{1 \\: 1} & c_{1 \\: 2} & \\dots & c_{1 \\: 27}\\\\\n",
    "c_{2 \\: 1} & c_{2 \\: 2} & \\dots & c_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "c_{32 \\: 1} & c_{32 \\: 2} & \\dots & c_{32 \\: 27}\\\\\n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "csi_{1} & csi_{1} & \\dots & csi_{1}\\\\\n",
    "csi_{2} & csi_{2} & \\dots & csi_{2}\\\\\n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\\n",
    "csi_{32} & csi_{32} & \\dots & csi_{32}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "c_{1 \\: 1} \\: csi_{1} & c_{1 \\: 2} \\: csi_{1} & \\dots & c_{1 \\: 27} \\: csi_{1}\\\\\n",
    "c_{2 \\: 1} \\: csi_{2} & c_{2 \\: 2} \\: csi_{2} & \\dots & c_{2 \\: 27} \\: csi_{2}\\\\\n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\\n",
    "c_{32 \\: 1} \\: csi_{32} & c_{32 \\: 2} \\: csi_{32} & \\dots & c_{32 \\: 27} \\: csi_{32}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First let's call this broadcasted tensor $\\mathbf{CSI}'$ and calculate its partial derivative.\n",
    "\n",
    "$$\n",
    "\\mathbf{CSI}' = \\begin{bmatrix}\n",
    "csi_{1} & csi_{1} & \\dots & csi_{1}\\\\\n",
    "csi_{2} & csi_{2} & \\dots & csi_{2}\\\\\n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\\n",
    "csi_{32} & csi_{32} & \\dots & csi_{32}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CSI}'} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}} \\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{CSI}'}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{P} = \\mathbf{C} \\circ \\mathbf{CSI}'$, $\\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{CSI}'}$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{CSI}'} = \\frac{\\partial}{\\partial \\mathbf{CSI}'} \\left( \\mathbf{C} \\circ \\mathbf{CSI}' \\right) = \\mathbf{C}\n",
    "$$\n",
    "\n",
    "Since the gradient of the vector $\\mathbf{CSI}$ is used multiple times (each column in the broadcast), we must sum each use of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92790ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cea05c",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `counts_sum`\n",
    "\n",
    "Note that `counts_sum_inv` is defined as\n",
    "\n",
    "```python\n",
    "counts_sum_inv = counts_sum**-1\n",
    "```\n",
    "\n",
    "Denoting `counts_sum_inv` as $\\mathbf{CSI}$ and `counts_sum` as $\\mathbf{CS}$\n",
    "\n",
    "$$\n",
    "\\mathbf{CSI} = \\mathbf{CS}^{-1}\n",
    "$$\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CS}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CSI}} \\frac{\\partial \\mathbf{CSI}}{\\partial \\mathbf{CS}}\n",
    "$$\n",
    "\n",
    "All we need to compute is $\\frac{\\partial \\mathbf{CSI}}{\\partial \\mathbf{CS}}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{CSI}}{\\partial \\mathbf{CS}} = \\frac{\\partial}{\\partial \\mathbf{CS}} \\left( \\mathbf{CS}^{-1} \\right) = - \\mathbf{CS}^{-2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13c5272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum = dcounts_sum_inv * - counts_sum.pow(-2)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb685980",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `counts`\n",
    "\n",
    "Note that `probs` is defined as\n",
    "\n",
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "and `counts_sum` is defined as\n",
    "\n",
    "```python\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "```\n",
    "\n",
    "Representing `probs` as $\\mathbf{P}$, `counts` as $\\mathbf{C}$, `counts_sum` as $\\mathbf{CS}$, and `counts_sum_inv` as $\\mathbf{CSI}$, we get two equations involving $\\mathbf{C}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{P} = \\mathbf{C} * \\mathbf{CSI}\\\\\n",
    "    \\mathbf{CS} = \\sum_{i} \\mathbf{C}_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To calculate $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}}$, we must calculate the gradient for both of the above equations and add their contributions to the gradient of $\\mathbf{C}$.\n",
    "\n",
    "1. Remember, the sizes of `counts` and `counts_sum_inv` are different, causing `counts_sum_inv` to be broadcasted.\n",
    "\n",
    "    Using the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}} \\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{C}}\n",
    "    $$\n",
    "\n",
    "    All we need to compute is $\\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{C}}$, denoting the broadcasted tensor of $\\mathbf{CSI}$ as $\\mathbf{CSI}'$.\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{C}} = \\frac{\\partial}{\\partial \\mathbf{C}} \\left( \\mathbf{C} \\circ \\mathbf{CSI}' \\right) = \\mathbf{CSI}'\n",
    "    $$\n",
    "\n",
    "2. Defining $\\mathbf{C}$ as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{C} = \\begin{bmatrix}\n",
    "    c_{1 \\: 1} & c_{1 \\: 2} & \\dots & c_{1 \\: 27}\\\\\n",
    "    c_{2 \\: 1} & c_{2 \\: 2} & \\dots & c_{2 \\: 27}\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    c_{32 \\: 1} & c_{32 \\: 2} & \\dots & c_{32 \\: 27}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    we know that $\\mathbf{CS}$ is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{CS} = \\begin{bmatrix}\n",
    "    c_{1 \\: 1} + c_{1 \\: 2} + \\dots + c_{1 \\: 27}\\\\\n",
    "    c_{2 \\: 1} + c_{2 \\: 2} + \\dots + c_{2 \\: 27}\\\\\n",
    "    \\vdots\\\\\n",
    "    c_{32 \\: 1} + c_{32 \\: 2} + \\dots + c_{32 \\: 27}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    Using the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CS}} \\frac{\\partial \\mathbf{CS}}{\\partial \\mathbf{C}}\n",
    "    $$\n",
    "\n",
    "    We only need to calculate $\\frac{\\partial \\mathbf{CS}}{\\partial \\mathbf{C}}$.\n",
    "\n",
    "    Since each element $c_{i j}$ of $\\mathbf{C}$ participates in a sum in some element of $\\mathbf{CS}$, the partial derivative $\\frac{\\partial \\mathbf{CS}_i}{\\partial \\mathbf{C}_{i j}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathbf{CS}_i}{\\partial \\mathbf{C}_{i j}} = \\frac{\\partial}{\\partial \\mathbf{C}_{i j}} \\left( c_{i \\, 1} + c_{i \\, 2} + \\dots + c_{i j} + \\dots c_{i \\, 27} \\right) = 1\n",
    "    $$\n",
    "\n",
    "    So, the partial derivative $\\frac{\\partial \\mathbf{CS}}{\\partial \\mathbf{C}}$ is a matrix the shape of $\\mathbf{C}$ with all elements being $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2f68d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts = (counts_sum_inv * dprobs) + (torch.ones_like(counts) * dcounts_sum)\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a3bb1",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `norm_logits`\n",
    "\n",
    "Note that `counts` is defined as\n",
    "\n",
    "```python\n",
    "counts = norm_logits.exp()\n",
    "```\n",
    "\n",
    "Representing `counts` as $\\mathbf{C}$ and `norm_logits` as $\\mathbf{NL}$, we have:\n",
    "\n",
    "$$\n",
    "\\mathbf{NL} = \\begin{bmatrix}\n",
    "nl_{1 \\: 1} & nl_{1 \\: 2} & \\dots & nl_{1 \\: 27}\\\\\n",
    "nl_{2 \\: 1} & nl_{2 \\: 2} & \\dots & nl_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "nl_{32 \\: 1} & nl_{32 \\: 2} & \\dots & nl_{32 \\: 27}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This way, $\\mathbf{C}$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\exp(\\mathbf{NL}) = \\begin{bmatrix}\n",
    "e^{nl_{1 \\: 1}} & e^{nl_{1 \\: 2}} & \\dots & e^{nl_{1 \\: 27}}\\\\\n",
    "e^{nl_{2 \\: 1}} & e^{nl_{2 \\: 2}} & \\dots & e^{nl_{2 \\: 27}}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "e^{nl_{32 \\: 1}} & e^{nl_{32 \\: 2}} & \\dots & e^{nl_{32 \\: 27}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{NL}}\n",
    "$$\n",
    "\n",
    "For each partial derivative $\\frac{\\partial \\mathbf{C}_{ij}}{\\partial \\mathbf{NL}_{ij}}$, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{C}_{ij}}{\\partial \\mathbf{NL}_{ij}} = \\frac{\\partial}{\\partial \\mathbf{NL}_{ij}}\\left( e^{nl_{ij}} \\right) = e^{nl_{ij}}\n",
    "$$\n",
    "\n",
    "Therefore, $\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{NL}} = \\mathbf{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5632a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dnorm_logits = counts * dcounts\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d853c25",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `logits_maxes`\n",
    "\n",
    "Note that `norm_logits` is defined as\n",
    "\n",
    "```python\n",
    "norm_logits = logits - logits_maxes\n",
    "```\n",
    "\n",
    "Since `logits` and `logits_maxes` are not the same shape, the operation will be broadcasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6421b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([32, 27]), logit_maxes.shape=torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f'{logits.shape=}, {logit_maxes.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f2a1a",
   "metadata": {},
   "source": [
    "This means the vector `logit_maxes` will be repeated $27$ times. So `norm_logits` is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{NL} = \\begin{bmatrix}\n",
    "l_{1 \\: 1} - lm_1 & l_{1 \\: 2} - lm_1 & \\dots & l_{1 \\: 27} - lm_1\\\\\n",
    "l_{2 \\: 1} - lm_2 & l_{2 \\: 2} - lm_2 & \\dots & l_{2 \\: 27} - lm_2\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "l_{32 \\: 1} - lm_{32} & l_{32 \\: 2} - lm_{32} & \\dots & l_{32 \\: 27} - lm_{32}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The gradient $\\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}}$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}} = \\begin{bmatrix}\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "\\vdots\\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Again using the chain rule, $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LM}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}} \\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}}$. Again, due to broadcasting, $\\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}}$ is repeated. Since all its entries are $-1$, we can sum across $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}}$ and negate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "558f2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogit_maxes = - dnorm_logits.sum(1, keepdim=True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4554c",
   "metadata": {},
   "source": [
    "Note that the logit normalization is performed to prevent floating point errors when values are too large. This means it should not have an effect on the loss, which means the gradient of `dlogit_maxes` should be approximately $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "891db07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0268e-09],\n",
       "        [-9.3132e-10],\n",
       "        [-1.8626e-09],\n",
       "        [-1.3970e-09],\n",
       "        [-2.7940e-09],\n",
       "        [ 2.3283e-10],\n",
       "        [-9.3132e-10],\n",
       "        [-2.7940e-09],\n",
       "        [ 1.3970e-09],\n",
       "        [-1.6298e-09],\n",
       "        [-3.7253e-09],\n",
       "        [-1.8626e-09],\n",
       "        [ 9.7789e-09],\n",
       "        [-6.5193e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [ 2.3283e-09],\n",
       "        [ 3.7253e-09],\n",
       "        [ 2.3283e-10],\n",
       "        [ 6.0536e-09],\n",
       "        [-2.7940e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [ 5.3551e-09],\n",
       "        [ 3.9581e-09],\n",
       "        [-1.8626e-09],\n",
       "        [ 1.3970e-09],\n",
       "        [-1.1642e-09],\n",
       "        [-2.7940e-09],\n",
       "        [-4.6566e-10],\n",
       "        [-6.7521e-09],\n",
       "        [-3.2596e-09],\n",
       "        [ 1.8626e-09]], grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37c813",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `logits`\n",
    "\n",
    "The tensor `logits` is used twice, so its gradient is a sum of the two separate gradients.\n",
    "\n",
    "```python\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "```\n",
    "\n",
    "1. The `logits` tensor $\\mathbf{L}$ is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{L} = \\begin{bmatrix}\n",
    "    l_{1 \\: 1} & l_{1 \\: 2} & \\dots & l_{1 \\: 27}\\\\\n",
    "    l_{2 \\: 1} & l_{2 \\: 2} & \\dots & l_{2 \\: 27}\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    l_{32 \\: 1} & l_{32 \\: 2} & \\dots & l_{32 \\: 27}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The `logit_maxes` tensor is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{LM} = \\begin{bmatrix}\n",
    "    lm_1\\\\\n",
    "    lm_2\\\\\n",
    "    \\vdots\\\\\n",
    "    lm_{32}\\\\\n",
    "    \\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\max(l_{1 \\: 1}, l_{1 \\: 2}, \\dots, l_{1 \\: 27})\\\\\n",
    "    \\max(l_{2 \\: 1}, l_{2 \\: 2}, \\dots, l_{2 \\: 27})\\\\\n",
    "    \\vdots\\\\\n",
    "    \\max(l_{32 \\: 1}, l_{32 \\: 2}, \\dots, l_{32 \\: 27})\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The partial derivative $\\frac{\\partial \\mathbf{LM}_{i}}{\\partial \\mathbf{L}_{i j}}$ is $1$ if $l_{i j}$ is the maximum of the row $l_{i \\, 1}, l_{i \\, 2}, \\dots l_{i \\, j}, \\dots, l_{i \\, 27}$, and is $0$ otherwise. From here we can use the chain rule $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LM}} \\frac{\\partial \\mathbf{LM}}{\\partial \\mathbf{L}}$\n",
    "\n",
    "2. The `norm_logits` tensor $\\mathbf{NL}$ is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{NL} = \\begin{bmatrix}\n",
    "    l_{1 \\: 1} - lm_1 & l_{1 \\: 2} - lm_1 & \\dots & l_{1 \\: 27} - lm_1\\\\\n",
    "    l_{2 \\: 1} - lm_2 & l_{2 \\: 2} - lm_2 & \\dots & l_{2 \\: 27} - lm_2\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    l_{32 \\: 1} - lm_{32} & l_{32 \\: 2} - lm_{32} & \\dots & l_{32 \\: 27} - lm_{32}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The partial derivative $\\frac{\\partial \\mathbf{NL}_{i j}}{\\partial \\mathbf{L}_{i j}}$ is $1$ because all $l_{i j}$ participate in the $\\mathbf{NL}$ matrix with coefficient $1$. This means\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{L}} = \\begin{bmatrix}\n",
    "    1 & 1 & \\dots & 1\\\\\n",
    "    1 & 1 & \\dots & 1\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    1 & 1 & \\dots & 1\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    From here we can use the chain rule $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}} \\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{L}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "986d420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogits = (F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes) + (dnorm_logits)\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd1867",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `h`, `W2`, and `b2`\n",
    "\n",
    "The `logits` tensor is defined as\n",
    "\n",
    "```python\n",
    "logits = h @ W2 + b2\n",
    "```\n",
    "\n",
    "Defining the $\\mathbf{H}$, $\\mathbf{W}_2$, and $\\mathbf{b}_2$ tensors as\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\begin{bmatrix}\n",
    "h_{1 \\: 1} & h_{1 \\: 2} & \\dots & h_{1 \\: 64}\\\\\n",
    "h_{2 \\: 1} & h_{2 \\: 2} & \\dots & h_{2 \\: 64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "h_{32 \\: 1} & h_{32 \\: 2} & \\dots & h_{32 \\: 64}\\\\\n",
    "\\end{bmatrix} \\quad\\quad \\mathbf{W}_2 = \\begin{bmatrix}\n",
    "w_{1 \\: 1} & w_{1 \\: 2} & \\dots & w_{1 \\: 27}\\\\\n",
    "w_{2 \\: 1} & w_{2 \\: 2} & \\dots & w_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "w_{64 \\: 1} & w_{64 \\: 2} & \\dots & w_{64 \\: 27}\\\\\n",
    "\\end{bmatrix} \\quad\\quad \\mathbf{b}_2 = \\begin{bmatrix}\n",
    "b_{1} & b_{2} & \\dots & b_{27}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The product $\\mathbf{H} \\mathbf{W}_2$ is performed as\n",
    "\n",
    "$$\n",
    "\\mathbf{H} \\mathbf{W}_2 = \\begin{bmatrix}\n",
    "h_{1 \\: 1} \\, w_{1 \\: 1} + h_{1 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 1} & h_{1 \\: 1} \\, w_{1 \\: 2} + h_{1 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 2} & \\dots & h_{1 \\: 1} \\, w_{1 \\: 27} + h_{1 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 27}\\\\\n",
    "h_{2 \\: 1} \\, w_{1 \\: 1} + h_{2 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 1} & h_{2 \\: 1} \\, w_{1 \\: 2} + h_{2 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 2} & \\dots & h_{2 \\: 1} \\, w_{1 \\: 27} + h_{2 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "h_{32 \\: 1} \\, w_{1 \\: 1} + h_{32 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 1} & h_{32 \\: 1} \\, w_{1 \\: 2} + h_{32 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 2} & \\dots & h_{32 \\: 1} \\, w_{1 \\: 27} + h_{32 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 27}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the full expression $\\mathbf{L} = \\mathbf{H} \\mathbf{W}_2 + \\mathbf{b}_2$, which causes $\\mathbf{b}_2$ to be broadcasted, is\n",
    "\n",
    "$$\n",
    "\\mathbf{L} = \\mathbf{H} \\mathbf{W}_2 + \\mathbf{b}_2 = \\begin{bmatrix}\n",
    "h_{1 \\: 1} \\, w_{1 \\: 1} + h_{1 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 1} + b_1 & h_{1 \\: 1} \\, w_{1 \\: 2} + h_{1 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 2} + b_2 & \\dots & h_{1 \\: 1} \\, w_{1 \\: 27} + h_{1 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 27} + b_{27}\\\\\n",
    "h_{2 \\: 1} \\, w_{1 \\: 1} + h_{2 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 1} + b_1 & h_{2 \\: 1} \\, w_{1 \\: 2} + h_{2 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 2} + b_2 & \\dots & h_{2 \\: 1} \\, w_{1 \\: 27} + h_{2 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 27} + b_{27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "h_{32 \\: 1} \\, w_{1 \\: 1} + h_{32 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 1} + b_{1} & h_{32 \\: 1} \\, w_{1 \\: 2} + h_{32 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 2} + b_{2} & \\dots & h_{32 \\: 1} \\, w_{1 \\: 27} + h_{32 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 27} + b_{27}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For $\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}}$, the partial derivative for entry $i,j$, $\\frac{\\partial \\mathbf{L}_{i j}}{\\partial \\mathbf{H}_{i j}}$, is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}_{i j}}{\\partial \\mathbf{H}_{i j}} = \\frac{\\partial}{\\partial h_{i j}} \\left( h_{i \\: 1} \\, w_{1 \\: i} + h_{i \\: 2} \\, w_{2 \\: i} + \\dots + h_{i \\: j} \\, w_{j \\: i} + \\dots + h_{i \\: 64} \\, w_{64 \\: i} + b_j \\right) = w_{j \\: i}\n",
    "$$\n",
    "\n",
    "which means the partial derivative $\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}}$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}} = {\\mathbf{W}_2}^\\mathrm{T}\n",
    "$$\n",
    "\n",
    "By similar logic, the partial derivative $\\frac{\\partial \\mathbf{L}}{\\partial {\\mathbf{W}_2}}$, is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial {\\mathbf{W}_2}}\n",
    "$$\n",
    "\n",
    "Lastly, for $\\mathbf{b}_2$, the partial derivative $\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{b}_2}$ for entry $i,j$, $\\frac{\\partial \\mathbf{L}_{i j}}{\\partial {\\mathbf{b}_2}_{i j}}$, is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}_{i j}}{\\partial {\\mathbf{b}_2}_{i j}} = \\frac{\\partial}{\\partial {b}_{j}} \\left( h_{i \\: 1} \\, w_{1 \\: i} + h_{i \\: 2} \\, w_{2 \\: i} + \\dots + h_{i \\: j} \\, w_{j \\: i} + \\dots + h_{i \\: 64} \\, w_{64 \\: i} + b_j \\right) = 1\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{b}_2$ is broadcasted, there are $64$ contributions to the gradient $\\frac{\\partial \\mathbf{L}}{\\partial {\\mathbf{b}_2}}$ (one for each row).\n",
    "\n",
    "From here, we can use the chain rule to calculate the derivatives of $\\mathbf{L}$ with respect to $\\mathbf{H}$, $\\mathbf{W}_2$, and $\\mathbf{b}_2$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{W}_2} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{W}_2}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{b}_2} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{b}_2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f93c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowing the shapes of the tensors involved in the matrix multiplication can help you figure out the correct way to multiply the gradients.\n",
      "logits.shape=torch.Size([32, 27]), W2.shape=torch.Size([64, 27]), h.shape=torch.Size([32, 64]), b2.shape=torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "print('Knowing the shapes of the tensors involved in the matrix multiplication can help you figure out the correct way to multiply the gradients.')\n",
    "print(f'{logits.shape=}, {W2.shape=}, {h.shape=}, {b2.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7433241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0, keepdim=True)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf08476",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `hpreact`\n",
    "\n",
    "The tensor `h` is defined as\n",
    "\n",
    "```python\n",
    "h = torch.tanh(hpreact)\n",
    "```\n",
    "\n",
    "Note the derivative of $\\mathrm{tanh}$\n",
    "\n",
    "$$\n",
    "y = \\mathrm{tanh}(x) \\Rightarrow \\frac{dy}{dx} = 1 - y^2\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{H} = \\mathrm{tanh}(\\mathbf{H}_{pre \\, activation})$, we know that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{H}}{\\partial \\mathbf{H}_{pre \\,activation}} = 1 - \\mathbf{H}^2\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}} \\frac{\\partial \\mathbf{H}}{\\partial \\mathbf{H}_{pre \\, activation}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e2dea48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "dhpreact = dh * (1.0 - h.pow(2))\n",
    "cmp('hpreact', dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6936812",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bngain`, `bnraw`, and `bnbias`\n",
    "\n",
    "The tensor `hpreact` is defined as\n",
    "\n",
    "```python\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```\n",
    "\n",
    "Note the shapes of each tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8716e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact.shape=torch.Size([32, 64]), bngain.shape=torch.Size([1, 64]), bnraw.shape=torch.Size([32, 64]), bnbias.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{hpreact.shape=}, {bngain.shape=}, {bnraw.shape=}, {bnbias.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fcb1d",
   "metadata": {},
   "source": [
    "Defining the ${\\mathbf{b}_{n}}_{gain}$, ${\\mathbf{B}_{n}}_{raw}$, and ${\\mathbf{b}_{n}}_{bias}$ tensors as\n",
    "\n",
    "$$\n",
    "{\\mathbf{b}_{n}}_{gain} = \\begin{bmatrix}\n",
    "{{b_n}_{gain}}_{1} & {{b_n}_{gain}}_{2} & \\dots & {{b_n}_{gain}}_{64}\n",
    "\\end{bmatrix} \\quad\\quad {\\mathbf{B}_{n}}_{raw} = \\begin{bmatrix}\n",
    "{{b_n}_{raw}}_{1 \\: 1} & {{b_n}_{raw}}_{1 \\: 2} & \\dots & {{b_n}_{raw}}_{1 \\: 64}\\\\\n",
    "{{b_n}_{raw}}_{2 \\: 1} & {{b_n}_{raw}}_{2 \\: 2} & \\dots & {{b_n}_{raw}}_{2 \\: 64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "{{b_n}_{raw}}_{32 \\: 1} & {{b_n}_{raw}}_{32 \\: 2} & \\dots & {{b_n}_{raw}}_{32 \\: 64}\\\\\n",
    "\\end{bmatrix} \\quad\\quad {\\mathbf{b}_{n}}_{bias} = \\begin{bmatrix}\n",
    "{{b_n}_{bias}}_{1} & {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{bias}}_{64}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The full operation $\\mathbf{H}_{pre \\, activation} = {\\mathbf{b}_{n}}_{gain} \\circ {\\mathbf{B}_{n}}_{raw} + {\\mathbf{b}_{n}}_{bias}$, including broadcasting, is\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_{pre \\, activation} = {\\mathbf{b}_{n}}_{gain} \\circ {\\mathbf{B}_{n}}_{raw} + {\\mathbf{b}_{n}}_{bias} = \\begin{bmatrix}\n",
    "{{b_n}_{gain}}_{1} \\: {{b_n}_{raw}}_{1 \\: 1} + {{b_n}_{bias}}_{1} & {{b_n}_{gain}}_{2} \\: {{b_n}_{raw}}_{1 \\: 2} + {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{gain}}_{64} \\: {{b_n}_{raw}}_{1 \\: 64} + {{b_n}_{bias}}_{64}\\\\\n",
    "{{b_n}_{gain}}_{1} \\: {{b_n}_{raw}}_{2 \\: 1} + {{b_n}_{bias}}_{1} & {{b_n}_{gain}}_{2} \\: {{b_n}_{raw}}_{2 \\: 2} + {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{gain}}_{64} \\: {{b_n}_{raw}}_{2 \\: 64} + {{b_n}_{bias}}_{64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "{{b_n}_{gain}}_{1} \\: {{b_n}_{raw}}_{32 \\: 1} + {{b_n}_{bias}}_{1} & {{b_n}_{gain}}_{2} \\: {{b_n}_{raw}}_{32 \\: 2} + {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{gain}}_{64} \\: {{b_n}_{raw}}_{32 \\: 64} + {{b_n}_{bias}}_{64}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculating the derivative $\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{gain}}}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}_{i j}}{\\partial {{\\mathbf{b}_{n}}_{gain}}_{i j}} = \\frac{\\partial}{\\partial {{{b}_{n}}_{gain}}_{j}} \\left( {{b_n}_{gain}}_{j} \\: {{b_n}_{raw}}_{i \\: j} + {{b_n}_{bias}}_{j} \\right) = {{b_n}_{raw}}_{i \\: j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{gain}}} = {\\mathbf{B}_{n}}_{raw}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Calculating the derivative $\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{B}_{n}}_{raw}}}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}_{i j}}{\\partial {{\\mathbf{B}_{n}}_{raw}}_{i j}} = \\frac{\\partial}{\\partial {{{b}_{n}}_{gain}}_{j}} \\left( {{b_n}_{gain}}_{j} \\: {{b_n}_{raw}}_{i \\: j} + {{b_n}_{bias}}_{j} \\right) = {{b_n}_{gain}}_{j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{B}_{n}}_{raw}}} = {\\mathbf{b}_{n}}_{gain}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Calculating the derivative $\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{bias}}}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}_{i j}}{\\partial {{\\mathbf{b}_{n}}_{bias}}_{i j}} = \\frac{\\partial}{\\partial {{{b}_{n}}_{gain}}_{j}} \\left( {{b_n}_{gain}}_{j} \\: {{b_n}_{raw}}_{i \\: j} + {{b_n}_{bias}}_{j} \\right) = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{bias}}} = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{b}_{n}}_{gain}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} \\frac{\\partial \\mathbf{H}_{pre \\, activation}}{\\partial {\\mathbf{b}_{n}}_{gain}}\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_{n}}_{raw}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} \\frac{\\partial \\mathbf{H}_{pre \\, activation}}{\\partial {\\mathbf{B}_{n}}_{raw}}\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{b}_{n}}_{bias}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} \\frac{\\partial \\mathbf{H}_{pre \\, activation}}{\\partial {\\mathbf{b}_{n}}_{bias}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1948637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bngain          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "dbnraw = dhpreact * bngain\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnbias', dbnbias, bnbias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba1534",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bnvar_inv`\n",
    "\n",
    "The tensor `bnraw` is defined as\n",
    "\n",
    "```python\n",
    "bnraw = bndiff * bnvar_inv\n",
    "```\n",
    "\n",
    "Note the shapes are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85f1d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnraw.shape=torch.Size([32, 64]), bndiff.shape=torch.Size([32, 64]), bnvar_inv.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{bnraw.shape=}, {bndiff.shape=}, {bnvar_inv.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ff900",
   "metadata": {},
   "source": [
    "So `bnvar_inv` will be broadcasted.\n",
    "\n",
    "We know how to do back-propagation for element-wise multiplication: we just multiply ${\\mathbf{B}_n}_{diff}$ by $\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{raw}}$ and sum across the rows (because of broadcasting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c655d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
     ]
    }
   ],
   "source": [
    "dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim=True)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3bc3d",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bnvar`\n",
    "\n",
    "The tensor `bnvar_inv` is defined as\n",
    "\n",
    "```python\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "```\n",
    "\n",
    "Taking the derivative \n",
    "\n",
    "$$\n",
    "\\frac{\\partial {{\\mathbf{B}_n}_{var}}_{inv}}{\\partial {\\mathbf{B}_n}_{var}} = \\frac{\\partial}{\\partial {\\mathbf{B}_n}_{var}} \\left( \\left( {\\mathbf{B}_n}_{var} + 1 \\times 10^{-5} \\right)^{-\\frac{1}{2}} \\right) = - \\frac{1}{2} \\left( {\\mathbf{B}_n}_{var} + 1 \\times 10^{-5} \\right)^{-\\frac{3}{2}}\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{var}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{B}_n}_{var}}_{inv}} \\frac{\\partial {{\\mathbf{B}_n}_{var}}_{inv}}{\\partial {\\mathbf{B}_n}_{var}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ef6ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar           | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n"
     ]
    }
   ],
   "source": [
    "dbnvar = dbnvar_inv * (-(1.0/2.0) * (bnvar + 1e-5)**(-3.0/2.0))\n",
    "cmp('bnvar', dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df660a",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bndiff2`\n",
    "\n",
    "The tensor `bnvar` is defined as\n",
    "\n",
    "```python\n",
    "bnvar = 1 / (batch_size-1) * bndiff2.sum(0, keepdim=True)\n",
    "```\n",
    "\n",
    "Let $m=32$ be the batch size. If we represent ${{\\mathbf{B}_n}_{diff}}_2$ as\n",
    "\n",
    "$$\n",
    "{{\\mathbf{B}_n}_{diff}}_2 = \\begin{bmatrix}\n",
    "{{{b_n}_{diff}}_2}_{\\: 1 \\, 1} & {{{b_n}_{diff}}_2}_{\\: 1 \\, 2} & \\dots & {{{b_n}_{diff}}_2}_{\\: 1 \\, 64}\\\\\n",
    "{{{b_n}_{diff}}_2}_{\\: 2 \\, 1} & {{{b_n}_{diff}}_2}_{\\: 2 \\, 2} & \\dots & {{{b_n}_{diff}}_2}_{\\: 2 \\, 64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "{{{b_n}_{diff}}_2}_{\\: 32 \\, 1} & {{{b_n}_{diff}}_2}_{\\: 32 \\, 2} & \\dots & {{{b_n}_{diff}}_2}_{\\: 32 \\, 64}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we define ${\\mathbf{b}_n}_{var}$ as\n",
    "\n",
    "$$\n",
    "{\\mathbf{b}_n}_{var} = \\begin{bmatrix}\n",
    "\\frac{1}{m-1} \\left( {{{b_n}_{diff}}_2}_{\\: 1 \\, 1} + {{{b_n}_{diff}}_2}_{\\: 2 \\, 1} + \\dots + {{{b_n}_{diff}}_2}_{\\: 32 \\, 1} \\right) & \\frac{1}{m-1} \\left( {{{b_n}_{diff}}_2}_{\\: 1 \\, 2} + {{{b_n}_{diff}}_2}_{\\: 2 \\, 2} + \\dots + {{{b_n}_{diff}}_2}_{\\: 32 \\, 2} \\right) & \\dots & \\frac{1}{m-1} \\left( {{{b_n}_{diff}}_2}_{\\: 1 \\, 64} + {{{b_n}_{diff}}_2}_{\\: 2 \\, 64} + \\dots + {{{b_n}_{diff}}_2}_{\\: 32 \\, 64} \\right)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Taking the partial derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial {{\\mathbf{b}_n}_{var}}_{i j}}{\\partial {{{\\mathbf{B}_n}_{diff}}_2}_{i j}} = \\frac{\\partial}{\\partial {{{{b}_n}_{diff}}_2}_{i j}} \\left( \\frac{1}{m-1} \\left( {{{b_n}_{diff}}_2}_{\\: 1 \\, j} + {{{b_n}_{diff}}_2}_{\\: 2 \\, j} + \\dots + {{{b_n}_{diff}}_2}_{\\: i \\, j} + \\dots + {{{b_n}_{diff}}_2}_{\\: 32 \\, j} \\right) \\right) = \\frac{1}{m-1}\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {{{\\mathbf{B}_n}_{diff}}_2}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{b}_n}_{var}}} \\frac{\\partial {{\\mathbf{b}_n}_{var}}}{\\partial {{{\\mathbf{B}_n}_{diff}}_2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc01e450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff2         | exact: False | approximate: True  | maxdiff: 4.3655745685100555e-11\n"
     ]
    }
   ],
   "source": [
    "dbndiff2 = dbnvar * ((1.0 / (batch_size - 1)) * torch.ones_like(bndiff2))\n",
    "cmp('bndiff2', dbndiff2, bndiff2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8f2d7",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bndiff`\n",
    "\n",
    "The tensor `bndiff` is used twice, once in the definition of `bnraw` and again in the definition of `bndiff2`.\n",
    "\n",
    "```python\n",
    "bnraw = bndiff * bnvar_inv\n",
    "bndiff2 = bndiff**2\n",
    "```\n",
    "\n",
    "1. The partial derivative $\\frac{\\partial {\\mathbf{B}_n}_{raw}}{\\partial {\\mathbf{B}_n}_{diff}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial {\\mathbf{B}_n}_{raw}}{\\partial {\\mathbf{B}_n}_{diff}} = {{\\mathbf{B}_n}_{var}}_{inv}\n",
    "    $$\n",
    "\n",
    "    From here we can use the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{diff}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{b}_n}_{raw}}} \\frac{\\partial {{\\mathbf{b}_n}_{raw}}}{\\partial {\\mathbf{B}_n}_{diff}}\n",
    "    $$\n",
    "\n",
    "    Note that this will be broadcasted because $\\frac{\\partial {\\mathbf{B}_n}_{raw}}{\\partial {\\mathbf{B}_n}_{diff}} = {{\\mathbf{B}_n}_{var}}_{inv}$ is a vector and $\\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{b}_n}_{raw}}}$ is a 2D matrix.\n",
    "\n",
    "2. The partial derivative $\\frac{\\partial {{\\mathbf{B}_n}_{diff}}_2}{\\partial {\\mathbf{B}_n}_{diff}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial {{\\mathbf{B}_n}_{diff}}_2}{\\partial {\\mathbf{B}_n}_{diff}} = \\frac{\\partial}{\\partial {\\mathbf{B}_n}_{diff}} \\left( {{\\mathbf{B}_n}_{diff}}^2 \\right) = 2 {{\\mathbf{B}_n}_{diff}}\n",
    "    $$\n",
    "\n",
    "    From here we can use the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{diff}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{B}_n}_{diff}}_2} \\frac{\\partial {{\\mathbf{B}_n}_{diff}}_2}{\\partial {\\mathbf{B}_n}_{diff}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78f46a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting the shapes of the tensors can also help\n",
      "bnraw.shape=torch.Size([32, 64]), bndiff.shape=torch.Size([32, 64]), bndiff2.shape=torch.Size([32, 64]), bnvar_inv.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print('Inspecting the shapes of the tensors can also help')\n",
    "print(f'{bnraw.shape=}, {bndiff.shape=}, {bndiff2.shape=}, {bnvar_inv.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b71f62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbndiff         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "dbndiff = ( dbnraw * bnvar_inv ) + ( dbndiff2 * (2 * bndiff) )\n",
    "cmp('dbndiff', dbndiff, bndiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde5326",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bnmeani`\n",
    "\n",
    "The tensor `bndiff` is defined as\n",
    "\n",
    "```python\n",
    "bndiff = hprebn - bnmeani\n",
    "```\n",
    "\n",
    "Note that `bndiff` and `hprebn` are  different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a27dcdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff.shape=torch.Size([32, 64]), bnmeani.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{bndiff.shape=}, {bnmeani.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75142b",
   "metadata": {},
   "source": [
    "So `bnmeani` will be broadcasted.\n",
    "\n",
    "Since this operation is a subtraction, the partial derivative $\\frac{\\partial {\\mathbf{B}_n}_{diff}}{\\partial {{\\mathbf{b}_n}_{mean}}_i}$ is $-1$. Then we use chain rule $\\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{b}_n}_{mean}}_i} = \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{diff}} \\frac{\\partial {\\mathbf{B}_n}_{diff}}{\\partial {{\\mathbf{b}_n}_{mean}}_i}$.\n",
    "\n",
    "Since `bnmeani` is broadcasted, we must perform a sum to add all the contributions to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca94e0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "dbnmeani = (dbndiff * (-1 * torch.ones_like(bndiff))).sum(0, keepdim=True)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33a358",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `hprebn`\n",
    "\n",
    "The tensor `hprebn` is used twice, once in the definition of `bndiff` and again in the definition of `bnmeani`.\n",
    "\n",
    "```python\n",
    "bndiff = hprebn - bnmeani\n",
    "bnmeani = 1.0 / batch_size * hprebn.sum(0, keepdim=True)\n",
    "```\n",
    "\n",
    "Note that `bndiff` and `hprebn` are the same sizes.\n",
    "\n",
    "1. The partial derivative $\\frac{\\partial {\\mathbf{B}_n}_{diff}}{\\partial {\\mathbf{H}_{pre}}_{b_n}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial {\\mathbf{B}_n}_{diff}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} = 1\n",
    "    $$\n",
    "\n",
    "    From here we can use the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{B}_n}_{diff}}} \\frac{\\partial {{\\mathbf{B}_n}_{diff}}}{\\partial {\\mathbf{H}_{pre}}_{b_n}}\n",
    "    $$\n",
    "\n",
    "2. The partial derivative $\\frac{\\partial {{\\mathbf{B}_n}_{mean}}_i}{\\partial {\\mathbf{H}_{pre}}_{b_n}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial {{\\mathbf{B}_n}_{mean}}_i}{\\partial {\\mathbf{H}_{pre}}_{b_n}} = \\frac{1}{m}\n",
    "    $$\n",
    "\n",
    "    where $m$ is the batch size. From here we can use the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{B}_n}_{mean}}_i} \\frac{\\partial {{\\mathbf{B}_n}_{mean}}_i}{\\partial {\\mathbf{H}_{pre}}_{b_n}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf3da1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "dhprebn = ( dbndiff.clone() ) + ( dbnmeani * ((1.0 / batch_size) * torch.ones_like(hprebn)) )\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae0b0c",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `embcat`, `W1`, and `b1`\n",
    "\n",
    "The `hprebn` tensor is defined as\n",
    "\n",
    "```python\n",
    "hprebn = embcat @ W1 + b1\n",
    "```\n",
    "\n",
    "Using the same logic as the matrix multiplication expression `logits = h @ W2 + b2`, we know that the gradients are\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{C}_{cat}} &= {\\mathbf{W}_1}^T\\\\\n",
    "\\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{W}_1} &= {\\mathbf{C}_{cat}}^T\\\\\n",
    "\\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{b}_1} &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{C}_{cat}$ is `embcat`, $\\mathbf{W}_1$ is `W1`, and $\\mathbf{b}_1$ is `b1`. Note that `b1` is broadcasted across the 0th dimension (down columns) due to its differing shape from `embcat @ W1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f5e48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn.shape=torch.Size([32, 64]), embcat.shape=torch.Size([32, 30]), W1.shape=torch.Size([30, 64]), b1.shape=torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{hprebn.shape=}, {embcat.shape=}, {W1.shape=}, {b1.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc750c7",
   "metadata": {},
   "source": [
    "From here, we can use the chain rule to calculate the partial derivatives of $\\mathscr{L}$ with respect to $\\mathbf{C}_{cat}$, $\\mathbf{W}_1$, and $\\mathbf{b}_1$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}_{cat}} &= \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} \\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{C}_{cat}}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{W}_1} &= \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} \\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{W}_1}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{b}_1} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{b}_1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "485e8b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0, keepdim=True)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68809dfb",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `emb`\n",
    "\n",
    "Note that `embcat` is jut a concatenation of `emb`, which in PyTorch is just a different view of the tensor.\n",
    "\n",
    "```python\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b26b8272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embcat.shape=torch.Size([32, 30]), emb.shape=torch.Size([32, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f'{embcat.shape=}, {emb.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa44d33",
   "metadata": {},
   "source": [
    "For the backward pass, we just undo this operation by viewing it as the shape of `emb`. Once we deal with the shape of the tensor, and knowing that $\\frac{\\partial \\mathbf{C}_{cat}}{\\partial \\mathbf{C}} = 1$, we can then use chain rule to get $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}_{cat}} \\frac{\\partial \\mathbf{C}_{cat}}{\\partial \\mathbf{C}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9d3a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n"
     ]
    }
   ],
   "source": [
    "demb = dembcat.view(emb.shape)\n",
    "cmp('emb', demb, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d6801",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `C`\n",
    "\n",
    "Note that the tensor `emb` is defined as\n",
    "\n",
    "```python\n",
    "emb = C[Xb]\n",
    "```\n",
    "\n",
    "Note what is happening in the example below. The tensor `Xb`, which is $32 \\times 3$, is picking out $3$ rows from `C`, and is doing so $32$ times. For each of the three rows it selects, it gets a vector of length $10$. This is why `emb` is $32 \\times 3 \\times 10$: there are $32$ examples, each with $3$ letters, and each letter represented by a vector with $10$ elements.\n",
    "\n",
    "Note that this only works if all elements in `Xb` are greater than or equal to $0$ and less than $27$, otherwise it would not be able to index `C` (because it is a $27 \\times 10$ tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "110f2c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.shape=torch.Size([32, 3, 10]), C.shape=torch.Size([27, 10]), Xb.shape=torch.Size([32, 3])\n",
      "tensor([1, 1, 4])\n",
      "tensor([[-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
      "          0.6772, -0.8404],\n",
      "        [-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
      "          0.6772, -0.8404],\n",
      "        [-0.9648, -0.2321, -0.3476,  0.3324, -1.3263,  1.1224,  0.5964,  0.4585,\n",
      "          0.0540, -1.7400]], grad_fn=<IndexBackward0>)\n",
      "tensor([-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
      "         0.6772, -0.8404], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.9648, -0.2321, -0.3476,  0.3324, -1.3263,  1.1224,  0.5964,  0.4585,\n",
      "         0.0540, -1.7400], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
      "          0.6772, -0.8404],\n",
      "        [-0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,  1.5618, -1.6261,\n",
      "          0.6772, -0.8404],\n",
      "        [-0.9648, -0.2321, -0.3476,  0.3324, -1.3263,  1.1224,  0.5964,  0.4585,\n",
      "          0.0540, -1.7400]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(f'{emb.shape=}, {C.shape=}, {Xb.shape=}')\n",
    "print(Xb[0])\n",
    "print(C[Xb[0]])\n",
    "print(C[1])\n",
    "print(C[4])\n",
    "print(torch.tensor([C[1].tolist(), C[1].tolist(), C[4].tolist()]))\n",
    "print(torch.tensor([C[1].tolist(), C[1].tolist(), C[4].tolist()]) == C[Xb[0].cpu()].cpu())\n",
    "print(torch.all(Xb >= 0))\n",
    "print(torch.all(Xb < 27))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b4aee",
   "metadata": {},
   "source": [
    "To back-propagate this operation, we need to \"route\" the gradient from `emb` to the indices of `C` that are chosen by `Xb`. The rest of the elements of `C` should have a gradient of 0.\n",
    "\n",
    "To do this, we initialize the gradient of `C` to all zeros and iterate through all the letters in `Xb`. For each letter we add the gradient from that letter in `emb` to the corresponding index in `C` (noting that the gradient is a vector of length $10$, since that is how letters are embedded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed748ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C               | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k, j]\n",
    "        dC[ix] += demb[k, j]\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b49347",
   "metadata": {},
   "source": [
    "## Exercise 2 - Back-Propagation Manually through the Entire Cross-Entropy Calculation\n",
    "\n",
    "We perform cross-entropy loss as such:\n",
    "\n",
    "```python\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "```\n",
    "\n",
    "For a single example, `logits` would be a $1 \\times 27$ vector $\\mathbf{\\ell}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{\\ell} = \\begin{bmatrix} \\ell_1 & \\ell_2 & \\dots & \\ell_{27} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get the counts vector $\\mathbf{c}$, we perform element-wise exponentiation.\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\begin{bmatrix} e^{\\ell_1} & e^{\\ell_2} & \\dots & e^{\\ell_{27}} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then to get probabilities, we divide each of the counts by the sum of the counts (softmax).\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\begin{bmatrix} \\frac{e^{\\ell_1}}{\\sum_{j=1}^{27} e^{\\ell_j}} & \\frac{e^{\\ell_2}}{\\sum_{j=1}^{27} e^{\\ell_j}} & \\dots & \\frac{e^{\\ell_{27}}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We then take the negative logarithm to get negative log likelihoods (NLLs).\n",
    "\n",
    "$$\n",
    "\\mathbf{p'} = \\begin{bmatrix} -\\log\\left(\\frac{e^{\\ell_1}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) & -\\log\\left(\\frac{e^{\\ell_2}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) & \\dots & -\\log\\left(\\frac{e^{\\ell_{27}}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the loss of this example is the NLL of the correct label. Let's call this label $y$, so the correct logit is $\\ell_y$, and the expression for loss, $\\mathscr{L}$, becomes\n",
    "\n",
    "$$\n",
    "\\mathscr{L} = -\\log\\left(\\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right)\n",
    "$$\n",
    "\n",
    "We want to find an expression for loss with respect to each logit, meaning $\\frac{\\partial \\mathscr{L}}{\\partial \\ell_i}$ for all $i = 1, 2, \\dots, 27$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= \\frac{\\partial}{\\partial \\ell_i} \\left( -\\log\\left(\\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) \\right)\\\\\n",
    "        &= - \\frac{\\partial}{\\partial \\ell_i} \\left( \\log\\left(\\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) \\right)\\\\\n",
    "        &= - \\frac{1}{\\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}}} \\frac{\\partial}{\\partial \\ell_i} \\left( \\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "        &= - \\frac{\\sum_{j=1}^{27} e^{\\ell_j}}{e^{\\ell_y}} \\frac{\\partial}{\\partial \\ell_i} \\left( \\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "        &= - \\frac{\\sum_{j=1}^{27} e^{\\ell_j}}{e^{\\ell_y}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_y} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} \\frac{\\partial}{\\partial \\ell_i} \\left( \\sum_{j=1}^{27} e^{\\ell_j} \\right)}{\\left( \\sum_{j=1}^{27} e^{\\ell_j} \\right)^2} \\right)\\\\\n",
    "        &= - \\frac{\\sum_{j=1}^{27} e^{\\ell_j}}{e^{\\ell_y}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_y} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} e^{\\ell_i}}{\\left( \\sum_{j=1}^{27} e^{\\ell_j} \\right)^2} \\right)\\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_y} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We now have to separate cases:\n",
    "\n",
    "- If $i = y$:\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= - \\frac{1}{e^{\\ell_i}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_i} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_i} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{1}{e^{\\ell_i}} \\left( \\frac{e^{\\ell_i} \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_i} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{\\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\\\\n",
    "            &= \\frac{e^{\\ell_i} - \\sum_{j=1}^{27} e^{\\ell_j}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\\\\n",
    "            &= \\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} - \\frac{\\sum_{j=1}^{27} e^{\\ell_j}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\\\\n",
    "        \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= \\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} - 1\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "    Note that $\\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}}$ is the definition of $p_i$ (from the softmax probability vector $\\mathbf{p}$).\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} = p_i - 1\n",
    "    $$\n",
    "\n",
    "- If $i \\ne y$:\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_y} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{0 \\cdot \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{0 - e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{- e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= \\frac{1}{e^{\\ell_y}} \\left( \\frac{e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "        \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= \\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\\\\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "    Again, note that $\\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}}$ is the definition of $p_i$ (from the softmax probability vector $\\mathbf{p}$).\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} = p_i\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e895d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\n"
     ]
    }
   ],
   "source": [
    "dlogits = F.softmax(logits, 1)      # Each logit[i,j] starts with gradient p[i,j]\n",
    "dlogits[range(batch_size), Yb] -= 1 # Subtract 1 from the correct labels\n",
    "dlogits /= batch_size               # Divide by the batch size since loss is averaged over batches\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03685a0a",
   "metadata": {},
   "source": [
    "Note that for each logit, its partial derivative is negative for the correct label and is positive, but very small, for the incorrect labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f7a519b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0745,  0.0857,  0.0193,  0.0522,  0.0195,  0.0817,  0.0242,  0.0353,\n",
       "        -0.9815,  0.0278,  0.0388,  0.0341,  0.0358,  0.0283,  0.0361,  0.0146,\n",
       "         0.0098,  0.0188,  0.0141,  0.0525,  0.0557,  0.0219,  0.0247,  0.0714,\n",
       "         0.0564,  0.0252,  0.0229], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084177ce",
   "metadata": {},
   "source": [
    "The sum across the partial derivative of the logits for each example is approximately $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "910e301d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9849e-10, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cffd7d0",
   "metadata": {},
   "source": [
    "So the gradient is kind of like a force for the logits, where the correct label pulls it in one direction, and the incorrect labels pull it slightly in the opposite direction, and the net force is approximately $0$. Since the neurons are all connected, eventually the force of pulling on these logits causes a pull on the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac320fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach().cpu(), cmap='gray')\n",
    "plt.title('Gradient of Logits')\n",
    "plt.xlabel('Character')\n",
    "plt.ylabel('Example in batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078d72f",
   "metadata": {},
   "source": [
    "Note also that we can reduce the code in the forward pass with the built-in PyTorch cross-entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17fbd95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually calculated loss: 3.3541, PyTorch cross-entropy loss: 3.3541, Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(f'Manually calculated loss: {loss.item():.4f}, PyTorch cross-entropy loss: {loss_fast.item():.4f}, Difference: {loss-loss_fast.abs().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3093a",
   "metadata": {},
   "source": [
    "## Exercise 3 - Back-Propagate through the Entire Batch Normalization Calculation\n",
    "\n",
    "We perform batch normalization as such:\n",
    "\n",
    "```python\n",
    "bnmeani = 1.0 / batch_size * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1 / (batch_size-1) * bndiff2.sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```\n",
    "\n",
    "Let `hprebn` (the output of $\\mathbf{C}_{cat} \\mathbf{W}_1 + \\mathbf{b}_1$, just before batch normalization) be $\\mathbf{x}$ and `hpreact` (the output of batch normalization, just before activation) be $\\mathbf{y}$. The vectors $\\mathbf{x}$ and $\\mathbf{y}$ are of size $m$, which is the batch size.\n",
    "\n",
    "Formally, the batch normalization is performed by first calculating the mean of $\\mathbf{x}$, which we call $\\mu_B$.\n",
    "\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
    "$$\n",
    "\n",
    "Then the variance of $\\mathbf{x}$ is calculated, referred to as $\\sigma_B^2$.\n",
    "\n",
    "$$\n",
    "\\sigma_B^2 = \\frac{1}{m-1} \\sum_{i=1}^m \\left( x_i - \\mu_B \\right)^2\n",
    "$$\n",
    "\n",
    "Note that dividing by $m-1$ instead of $m$ is Bessel's correction.\n",
    "\n",
    "We then calculate `bnraw`, which we refer to as $\\hat{\\mathbf{x}}$. For each $i=1,2,\\dots,m$ we calculate $\\hat{x}_i$ as\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a small positive number ($1 \\times 10^{-5}$ in our case).\n",
    "\n",
    "Finally, we calculate $y_i$ for $i=1,2,\\dots,m$ as\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "where $\\gamma$ and $\\beta$ are the gain and bias vectors, respectively.\n",
    "\n",
    "We want to find $\\frac{\\partial \\mathscr{L}}{\\partial x_i}$, given that we have $\\frac{\\partial \\mathscr{L}}{\\partial y_i}$. We can back-propagate our way toward $\\mathbf{x}$ on step at a time, starting at $\\hat{\\mathbf{x}}$.\n",
    "\n",
    "First, we want to find $\\frac{\\partial \\mathscr{L}}{\\partial \\hat{x}_i}$, so we can use the chain rule to note that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\hat{x}_i} = \\frac{\\partial \\mathscr{L}}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\hat{x}_i}\n",
    "$$\n",
    "\n",
    "Calculating $\\frac{\\partial y_i}{\\partial \\hat{x}_i}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial \\hat{x}_i} = \\frac{\\partial}{\\partial \\hat{x}_i} \\left( \\gamma \\hat{x}_i + \\beta \\right) = \\gamma\n",
    "$$\n",
    "\n",
    "So we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\hat{x}_i} = \\frac{\\partial \\mathscr{L}}{\\partial y_i} \\gamma\n",
    "$$\n",
    "\n",
    "Next, we can find $\\frac{\\partial \\mathscr{L}}{\\partial \\sigma_B^2}$ using the chain rule, and remembering to sum the contributions to the gradient because $\\sigma_B$ is a scalar while $\\hat{\\mathbf{x}}$ is a vector.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\sigma_B^2} = \\sum_{i=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial \\sigma_B^2} \\right)\n",
    "$$\n",
    "\n",
    "Calculating $\\frac{\\partial \\hat{x}_i}{\\partial \\sigma_B^2}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\hat{x}_i}{\\partial \\sigma_B^2} &= \\frac{\\partial}{\\partial \\sigma_B^2} \\left( \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right)\\\\\n",
    "        &= \\frac{\\partial}{\\partial \\sigma_B^2} \\left( \\left( x_i - \\mu_B \\right) \\left( \\sigma_B^2 + \\epsilon \\right)^{-\\frac{1}{2}} \\right)\\\\\n",
    "        &= \\left( x_i - \\mu_B \\right) \\frac{\\partial}{\\partial \\sigma_B^2} \\left(  \\left( \\sigma_B^2 + \\epsilon \\right)^{-\\frac{1}{2}} \\right)\\\\\n",
    "        &= \\left( x_i - \\mu_B \\right) \\cdot - \\frac{1}{2} \\left( \\sigma_B^2 + \\epsilon \\right)^{-\\frac{3}{2}} \\frac{\\partial}{\\partial \\sigma_B^2} \\left( \\sigma_B^2 + \\epsilon \\right)\\\\\n",
    "        &= \\left( x_i - \\mu_B \\right) \\cdot - \\frac{1}{2} \\left( \\sigma_B^2 + \\epsilon \\right)^{-\\frac{3}{2}} \\cdot 1\\\\\n",
    "    \\frac{\\partial \\hat{x}_i}{\\partial \\sigma_B^2} &= - \\frac{\\left( x_i - \\mu_B \\right)}{2 \\left( \\sigma_B^2 + \\epsilon \\right)^{\\frac{3}{2}}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we can express $\\frac{\\partial \\mathscr{L}}{\\partial \\sigma_B^2}$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\sigma_B^2} &= \\sum_{i=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial \\sigma_B^2} \\right)\\\\\n",
    "        &= \\sum_{i=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} \\gamma \\cdot - \\frac{\\left( x_i - \\mu_B \\right)}{2 \\left( \\sigma_B^2 + \\epsilon \\right)^{\\frac{3}{2}}} \\right)\\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\sigma_B^2} &= -\\frac{1}{2} \\gamma \\sum_{i=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} \\frac{\\left( x_i - \\mu_B \\right)}{\\left( \\sigma_B^2 + \\epsilon \\right)^{\\frac{3}{2}}} \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We follow a similar process for $\\frac{\\partial \\mathscr{L}}{\\partial \\mu_B^2}$. The scalar $\\mu_B$ is used $m$ times by $\\hat{\\mathbf{x}}$ (once for each $\\hat{x}_i$) and once by $\\sigma_B^2$. All of these contributions sum up to get the full expression for the gradient of $\\mu_B$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mu_B} = \\sum_{i=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial \\mu_B} \\right) + \\frac{\\partial \\mathscr{L}}{\\partial \\sigma_B^2} \\frac{\\partial \\sigma_B^2}{\\partial \\mu_B}\n",
    "$$\n",
    "\n",
    "Calculating $\\frac{\\partial \\hat{x}_i}{\\partial \\mu_B}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\hat{x}_i}{\\partial \\mu_B} &= \\frac{\\partial}{\\partial \\mu_B} \\left( \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right) = - \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Calculating $\\frac{\\partial \\sigma_B^2}{\\partial \\mu_B}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\sigma_B^2}{\\partial \\mu_B} &= \\frac{\\partial}{\\partial \\mu_B} \\left( \\frac{1}{m-1} \\sum_{i=1}^m \\left( x_i - \\mu_B \\right)^2 \\right)\\\\\n",
    "        &= \\frac{\\partial}{\\partial \\mu_B} \\left( \\frac{1}{m-1} \\left( \\left( x_1 - \\mu_B \\right)^2 + \\left( x_2 - \\mu_B \\right)^2 + \\dots + \\left( x_m - \\mu_B \\right)^2 \\right) \\right)\\\\\n",
    "        &= \\frac{1}{m-1} \\left( \\frac{\\partial}{\\partial \\mu_B} \\left( \\left( x_1 - \\mu_B \\right)^2 + \\left( x_2 - \\mu_B \\right)^2 + \\dots + \\left( x_m - \\mu_B \\right)^2 \\right) \\right)\\\\\n",
    "        &= \\frac{1}{m-1} \\left( \\frac{\\partial}{\\partial \\mu_B} \\left( x_1 - \\mu_B \\right)^2 + \\frac{\\partial}{\\partial \\mu_B} \\left( x_2 - \\mu_B \\right)^2 + \\dots + \\frac{\\partial}{\\partial \\mu_B} \\left( x_m - \\mu_B \\right)^2 \\right)\\\\\n",
    "        &= \\frac{1}{m-1} \\left( 2 \\left( x_1 - \\mu_B \\right) \\frac{\\partial}{\\partial \\mu_B} \\left( x_1 - \\mu_B \\right) + 2 \\left( x_2 - \\mu_B \\right) \\frac{\\partial}{\\partial \\mu_B} \\left( x_2 - \\mu_B \\right) + \\dots + 2 \\left( x_m - \\mu_B \\right) \\frac{\\partial}{\\partial \\mu_B} \\left( x_m - \\mu_B \\right) \\right)\\\\\n",
    "        &= \\frac{1}{m-1} \\left( 2 \\left( x_1 - \\mu_B \\right) + 2 \\left( x_2 - \\mu_B \\right) + \\dots + 2 \\left( x_m - \\mu_B \\right) \\right)\\\\\n",
    "        &= \\frac{2}{m-1} \\left( \\left( x_1 - \\mu_B \\right) + \\left( x_2 - \\mu_B \\right) + \\dots + \\left( x_m - \\mu_B \\right) \\right)\\\\\n",
    "        &= \\frac{2}{m-1} \\left( x_1 + x_2 + \\dots + x_m - \\mu_B - \\mu_B - \\dots - \\mu_B \\right)\\\\\n",
    "    \\frac{\\partial \\sigma_B^2}{\\partial \\mu_B} &= \\frac{2}{m-1} \\left( \\sum_{i=1}^m x_i - m \\mu_B \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that since $\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i$, we also have $m \\mu_B = \\sum_{i=1}^m x_i$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\sigma_B^2}{\\partial \\mu_B} &= \\frac{2}{m-1} \\left( m \\mu_B - m \\mu_B \\right)\\\\\n",
    "    \\frac{\\partial \\sigma_B^2}{\\partial \\mu_B} &= 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Putting the components for the gradient of $\\mu_B$ together:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mu_B} &= \\sum_{i=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial \\mu_B} \\right) + \\frac{\\partial \\mathscr{L}}{\\partial \\sigma_B^2} \\frac{\\partial \\sigma_B^2}{\\partial \\mu_B}\\\\\n",
    "    &= \\sum_{i=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} \\gamma \\left( - \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right) \\right) + 0\\\\\n",
    " \\frac{\\partial \\mathscr{L}}{\\partial \\mu_B} &= - \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\sum_{i=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Finally, we can calculate $\\frac{\\partial \\mathscr{L}}{\\partial x_i}$. Note that the scalars $\\mu_B$ and $\\sigma_B^2$ depend on each $x_i$. And, since $\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$, each $\\hat{x}_i$ depends on $x_i$. So for each $x_i$, for $i=1,2,\\dots,m$, there are three contributions to the gradient.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial x_i} = \\frac{\\partial \\mathscr{L}}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial x_i} + \\frac{\\partial \\mathscr{L}}{\\partial \\mu_B} \\frac{\\partial \\mu_B}{\\partial x_i} + \\frac{\\partial \\mathscr{L}}{\\partial \\sigma_B^2} \\frac{\\partial \\sigma_B^2}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "Calculating $\\frac{\\partial \\hat{x}_i}{\\partial x_i}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{x}_i}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left( \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right) = \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Calculating $\\frac{\\partial \\mu_B}{\\partial x_i}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mu_B}{\\partial x_i} &= \\frac{\\partial}{\\partial x_i} \\left( \\frac{1}{m} \\sum_{i=1}^m x_i \\right)\\\\\n",
    "        &= \\frac{1}{m} \\frac{\\partial}{\\partial x_i} \\left( x_1 + x_2 + \\dots + x_i + \\dots + x_m \\right)\\\\\n",
    "        &= \\frac{1}{m} \\cdot \\left( 0 + 0 + \\dots + 1 + \\dots + 0 \\right)\\\\\n",
    "        &= \\frac{1}{m} \\cdot 1\\\\\n",
    "    \\frac{\\partial \\mu_B}{\\partial x_i} &= \\frac{1}{m}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Calculating $\\frac{\\partial \\sigma_B^2}{\\partial x_i}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\sigma_B^2}{\\partial x_i} &= \\frac{\\partial}{\\partial x_i} \\left( \\frac{1}{m-1} \\sum_{i=1}^m \\left( x_i - \\mu_B \\right)^2 \\right)\\\\\n",
    "        &= \\frac{1}{m-1} \\frac{\\partial}{\\partial x_i} \\left( \\sum_{i=1}^m \\left( x_i - \\mu_B \\right)^2 \\right)\\\\\n",
    "        &= \\frac{1}{m-1} \\frac{\\partial}{\\partial x_i} \\left( \\left( x_1 - \\mu_B \\right)^2 + \\left( x_2 - \\mu_B \\right)^2 + \\dots + \\left( x_i - \\mu_B \\right)^2 + \\dots + \\left( x_m - \\mu_B \\right)^2 \\right)\\\\\n",
    "        &= \\frac{1}{m-1} \\left( 0 + 0 + \\dots + \\frac{\\partial}{\\partial x_i} \\left( x_i - \\mu_B \\right)^2 + \\dots + 0 \\right)\\\\\n",
    "        &= \\frac{1}{m-1} \\left( \\frac{\\partial}{\\partial x_i} \\left( x_i - \\mu_B \\right)^2 \\right)\\\\\n",
    "        &= \\frac{1}{m-1} \\left( 2 \\left( x_i - \\mu_B \\right) \\right)\\\\\n",
    "    \\frac{\\partial \\sigma_B^2}{\\partial x_i} &= \\frac{2 \\left( x_i - \\mu_B \\right)}{m-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Therefore, $\\frac{\\partial \\mathscr{L}}{\\partial x_i}$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial x_i} &= \\frac{\\partial \\mathscr{L}}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial x_i} + \\frac{\\partial \\mathscr{L}}{\\partial \\mu_B} \\frac{\\partial \\mu_B}{\\partial x_i} + \\frac{\\partial \\mathscr{L}}{\\partial \\sigma_B^2} \\frac{\\partial \\sigma_B^2}{\\partial x_i}\\\\\n",
    "        &= \\frac{\\partial \\mathscr{L}}{\\partial y_i} \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\left( - \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) \\right) \\frac{1}{m} + \\left( -\\frac{1}{2} \\gamma \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\frac{\\left( x_j - \\mu_B \\right)}{\\left( \\sigma_B^2 + \\epsilon \\right)^{\\frac{3}{2}}} \\right) \\right) \\frac{2 \\left( x_i - \\mu_B \\right)}{m-1}\\\\\n",
    "        &= \\frac{\\partial \\mathscr{L}}{\\partial y_i} \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} - \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) \\frac{1}{m} - \\frac{1}{2} \\gamma \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\frac{\\left( x_j - \\mu_B \\right)}{\\left( \\sigma_B^2 + \\epsilon \\right)^{\\frac{3}{2}}} \\right) \\frac{2 \\left( x_i - \\mu_B \\right)}{m-1}\\\\\n",
    "        &= \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} - \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) \\frac{1}{m} - \\frac{1}{2} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\frac{\\left( x_j - \\mu_B \\right)}{\\sigma_B^2 + \\epsilon} \\right) \\frac{2 \\left( x_i - \\mu_B \\right)}{m-1} \\right)\\\\\n",
    "        &= \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} - \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) \\frac{1}{m} - \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\frac{\\left( x_j - \\mu_B \\right)}{\\sigma_B^2 + \\epsilon} \\right) \\frac{\\left( x_i - \\mu_B \\right)}{m-1} \\right)\\\\\n",
    "        &= \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} - \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) \\frac{1}{m} - \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\frac{\\left( x_j - \\mu_B \\right)}{\\sqrt{\\sigma_B^2 + \\epsilon}\\sqrt{\\sigma_B^2 + \\epsilon}} \\right) \\frac{\\left( x_i - \\mu_B \\right)}{m-1} \\right)\\\\\n",
    "        &= \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} - \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) \\frac{1}{m} - \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\frac{\\left( x_j - \\mu_B \\right)}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right) \\frac{\\left( x_i - \\mu_B \\right)}{(m-1)\\sqrt{\\sigma_B^2 + \\epsilon}} \\right)\\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial x_i} &= \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} - \\frac{1}{m} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) - \\frac{1}{m-1} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\frac{\\left( x_j - \\mu_B \\right)}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right) \\frac{\\left( x_i - \\mu_B \\right)}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since $\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial x_i} &= \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} - \\frac{1}{m} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) - \\frac{1}{m-1} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\hat{x_j} \\right) \\hat{x_i} \\right)\\\\\n",
    "        &= \\gamma \\frac{1}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_i} - \\frac{1}{m} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) - \\frac{m}{(m-1)m} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\hat{x_j} \\right) \\hat{x_i} \\right)\\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial x_i} &= \\gamma \\frac{1}{m \\sqrt{\\sigma_B^2 + \\epsilon}} \\left( m \\frac{\\partial \\mathscr{L}}{\\partial y_i} - \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\right) - \\frac{m}{m-1} \\sum_{j=1}^m \\left( \\frac{\\partial \\mathscr{L}}{\\partial y_j} \\hat{x_j} \\right) \\hat{x_i} \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8171e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n"
     ]
    }
   ],
   "source": [
    "dhprebn = bngain * (1.0 / batch_size) * bnvar_inv * ( batch_size * dhpreact - dhpreact.sum(0) - (batch_size / (batch_size - 1)) * (dhpreact * bnraw).sum(0) * bnraw )\n",
    "cmp('dhprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c9d15",
   "metadata": {},
   "source": [
    "Note that we can also reduce the batch normalization code down to one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0fbd08ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum difference between hpreact and hpreact_fast: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print(f'Maximum difference between hpreact and hpreact_fast: {(hpreact - hpreact_fast).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316f2e2",
   "metadata": {},
   "source": [
    "## Exercise 4 - Training the Neural Network with Manual Back-Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bee3a793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g, device=device)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g, device=device) * (5.0/3.0) / (math.sqrt(n_embd * block_size))\n",
    "b1 = torch.randn(n_hidden,                        generator=g, device=device) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g, device=device) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g, device=device) * 0.1\n",
    "\n",
    "bngain = torch.randn((1, n_hidden), device=device) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden), device=device) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f'Number of parameters: {sum(p.nelement() for p in parameters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "082ea128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/200000, loss: 3.2880027294158936\n",
      "Iteration 8000/200000, loss: 2.5092251300811768\n",
      "Iteration 16000/200000, loss: 2.515756130218506\n",
      "Iteration 24000/200000, loss: 2.2205543518066406\n",
      "Iteration 32000/200000, loss: 1.9139076471328735\n",
      "Iteration 40000/200000, loss: 2.3487486839294434\n",
      "Iteration 48000/200000, loss: 2.431751012802124\n",
      "Iteration 56000/200000, loss: 2.493825674057007\n",
      "Iteration 64000/200000, loss: 2.1253273487091064\n",
      "Iteration 72000/200000, loss: 2.160828113555908\n",
      "Iteration 80000/200000, loss: 2.0222008228302\n",
      "Iteration 88000/200000, loss: 2.1858842372894287\n",
      "Iteration 96000/200000, loss: 2.4489762783050537\n",
      "Iteration 104000/200000, loss: 2.434293746948242\n",
      "Iteration 112000/200000, loss: 2.080416679382324\n",
      "Iteration 120000/200000, loss: 1.9263767004013062\n",
      "Iteration 128000/200000, loss: 1.8950459957122803\n",
      "Iteration 136000/200000, loss: 2.336491823196411\n",
      "Iteration 144000/200000, loss: 2.154329538345337\n",
      "Iteration 152000/200000, loss: 2.1447949409484863\n",
      "Iteration 160000/200000, loss: 2.4997127056121826\n",
      "Iteration 168000/200000, loss: 2.1168100833892822\n",
      "Iteration 176000/200000, loss: 1.8595435619354248\n",
      "Iteration 184000/200000, loss: 1.9521340131759644\n",
      "Iteration 192000/200000, loss: 1.9190980195999146\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 200_000\n",
    "lossi = []\n",
    "\n",
    "# No need to have PyTorch keep track of gradients since we will do so manually\n",
    "with torch.no_grad():\n",
    "    for i in range(max_steps):\n",
    "\n",
    "        # --- Batch ---\n",
    "        ix = torch.randint(0, X_tr.shape[0], (batch_size,), generator=g, device=device)\n",
    "        Xb, Yb = X_tr[ix], Y_tr[ix]\n",
    "\n",
    "        # --- Forward pass ---\n",
    "        # Embedding vector from lookup table C\n",
    "        emb = C[Xb]\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        # First layer\n",
    "        hprebn = embcat @ W1 + b1\n",
    "        # Batch normalization\n",
    "        bnmeani = 1.0 / batch_size * hprebn.sum(0, keepdim=True)\n",
    "        bndiff = hprebn - bnmeani\n",
    "        bndiff2 = bndiff**2\n",
    "        bnvar = 1 / (batch_size-1) * bndiff2.sum(0, keepdim=True)\n",
    "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "        bnraw = bndiff * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact)\n",
    "        # Second layer\n",
    "        logits = h @ W2 + b2\n",
    "        # Loss calculation\n",
    "        loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "        # --- Backward pass ---\n",
    "        # Initialize zero gradients\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        # Softmax layer back-propagation\n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(batch_size), Yb] -= 1\n",
    "        dlogits /= batch_size\n",
    "        # Second layer back-propagation\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "        # Non-linearity back-propagation\n",
    "        dhpreact = dh * (1.0 - h.pow(2))\n",
    "        # Batch normalization back-propagation\n",
    "        dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "        dhprebn = bngain * (1.0 / batch_size) * bnvar_inv * ( batch_size * dhpreact - dhpreact.sum(0) - (batch_size / (batch_size - 1)) * (dhpreact * bnraw).sum(0) * bnraw )\n",
    "        # First layer back-propagation\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        # Embedding layer back-propagation\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k, j]\n",
    "                dC[ix] += demb[k, j]\n",
    "\n",
    "        # --- Parameter updates ---\n",
    "        gradients: List[Tensor] = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "        lr = 0.1 if i < max_steps / 2 else 0.01\n",
    "        for p, grad in zip(parameters, gradients):\n",
    "            p.data += -lr * grad\n",
    "\n",
    "        if max_steps < 25 or (i % (max_steps // 25)) == 0:\n",
    "            print(f'Iteration {i}/{max_steps}, loss: {loss.item()}')\n",
    "        lossi.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1893cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate batch normalization parameters (bnmean and bnvar) after training\n",
    "with torch.no_grad():\n",
    "    emb = C[X_tr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f15fae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def loss_of_split(split: Literal['train'] | Literal['test'] | Literal['val']):\n",
    "    x, y = {\n",
    "        'train': (X_tr, Y_tr),\n",
    "        'val': (X_dev, Y_dev),\n",
    "        'test': (X_te, Y_te)\n",
    "    }[split]\n",
    "\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    h_pre_act = embcat @ W1 + b1\n",
    "    h_pre_act = bngain * (h_pre_act - bnmean) / (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(h_pre_act)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    print(f'Loss of \\'{split}\\' split: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71700c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of 'train' split: 2.4527\n",
      "Loss of 'val' split: 2.4797\n"
     ]
    }
   ],
   "source": [
    "loss_of_split('train')\n",
    "loss_of_split('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ae5fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kan.\n",
      "minor.\n",
      "slea.\n",
      "emmena.\n",
      "ridgianaleighani.\n",
      "liana.\n",
      "kenslam.\n",
      "jara.\n",
      "caylonna.\n",
      "karyan.\n",
      "ediet.\n",
      "juu.\n",
      "elouelahiyah.\n",
      "kalynn.\n",
      "torevonjonis.\n",
      "ismeya.\n",
      "juon.\n",
      "issietteric.\n",
      "dylintzion.\n",
      "rura.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator(device=device).manual_seed(TORCH_GENERATOR_SEED + 1)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context], device=device)]\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(str().join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
