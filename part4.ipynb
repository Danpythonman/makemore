{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4751d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import List, Literal\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ef3138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using CUDA')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e2af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TORCH_GENERATOR_SEED = 2147483647\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "g = torch.Generator(device=device).manual_seed(TORCH_GENERATOR_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917eec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51d88e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b1f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(str().join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d112dc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bbb8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size, device):\n",
    "    X_data, Y_data = [], []\n",
    "    for word in words:\n",
    "        context = [0 for _ in range(block_size)]\n",
    "        for ch in word + '.':\n",
    "            ix = stoi[ch]\n",
    "\n",
    "            X_data.append(context)\n",
    "            Y_data.append(ix)\n",
    "\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X_data, device=device)\n",
    "    Y = torch.tensor(Y_data, device=device)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f07c9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr.shape=torch.Size([182625, 3]), Y_tr.shape=torch.Size([182625])\n",
      "X_dev.shape=torch.Size([22655, 3]), Y_dev.shape=torch.Size([22655])\n",
      "X_te.shape=torch.Size([22866, 3]), Y_te.shape=torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "X_tr, Y_tr = build_dataset(words[:n1], block_size, device)\n",
    "X_dev, Y_dev = build_dataset(words[n1:n2], block_size, device)\n",
    "X_te, Y_te = build_dataset(words[n2:], block_size, device)\n",
    "\n",
    "print(f'{X_tr.shape=}, {Y_tr.shape=}\\n{X_dev.shape=}, {Y_dev.shape=}\\n{X_te.shape=}, {Y_te.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953c6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b5c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g, device=device)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g, device=device) * (5.0/3.0) / (math.sqrt(n_embd * block_size))\n",
    "b1 = torch.randn(n_hidden,                        generator=g, device=device) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g, device=device) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g, device=device) * 0.1\n",
    "\n",
    "bngain = torch.randn((1, n_hidden), device=device) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden), device=device) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f'Number of parameters: {sum(p.nelement() for p in parameters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5b4718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "ix = torch.randint(0, X_tr.shape[0], (batch_size,), generator=g, device=device)\n",
    "Xb, Yb = X_tr[ix], Y_tr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "382b4b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3615, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "hprebn = embcat @ W1 + b1\n",
    "\n",
    "bnmeani = 1.0 / batch_size * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1 / (batch_size-1) * bndiff2.sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes               # Subtract by max for numerical safety\n",
    "\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv                 # Multiplying by inverse instead of dividing\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0b965b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(batch_size),Yb].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef776fa",
   "metadata": {},
   "source": [
    "## Exercise 1 - Back-Propagation Manually through each Intermediate Step\n",
    "\n",
    "### Derivative of `loss` with respect to `logprobs`.\n",
    "\n",
    "The `logprobs` tensor has dimension $32 \\times 27$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338d4057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532848f",
   "metadata": {},
   "source": [
    "Let $m = 32$ be the batch size, $n = 27$ be the vocabulary size, and $\\mathbf{LP}$ be the `logprobs` tensor.\n",
    "\n",
    "$$\n",
    "\\mathbf{LP} = \\begin{bmatrix}\n",
    "lp_{1 \\, 1} & lp_{1 \\, 2} & \\dots & lp_{1 \\, n}\\\\\n",
    "lp_{2 \\, 1} & lp_{2 \\, 2} & \\dots & lp_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "lp_{m \\, 1} & lp_{m \\, 2} & \\dots & lp_{m \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let $\\mathscr{L}$ be the loss. We want to find $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}}$.\n",
    "\n",
    "We calculate loss as\n",
    "\n",
    "```python\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "```\n",
    "\n",
    "So let's first find an expression for `-logprobs[range(batch_size), Yb]`. We call this tensor $\\mathbf{LP_b}$. We index the rows with `range(batch_size)`, which means we get all rows $0, 1, \\dots, m$. And for each row we index that row with `Yb`. This means the size of `Yb` must be the batch size. Indeed it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18367560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(range(batch_size))=32, Yb.shape=torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(range(batch_size))=}, {Yb.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066238d",
   "metadata": {},
   "source": [
    "The vector $\\mathbf{LP}_b$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\mathbf{LP}_b = \\begin{bmatrix}\n",
    "    \\mathbf{LP}_{1 \\; {\\mathbf{y}_b}_1}\\\\\n",
    "    \\mathbf{LP}_{2 \\; {\\mathbf{y}_b}_2}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\mathbf{LP}_{m \\; {\\mathbf{y}_b}_m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the loss is the negative mean of the elements in this vector.\n",
    "\n",
    "$$\n",
    "\\mathscr{L} = - \\frac{{\\mathbf{y}_b}_1 + {\\mathbf{y}_b}_2 + \\dots + {\\mathbf{y}_b}_m}{m}\n",
    "$$\n",
    "\n",
    "For the elements of $\\mathbf{LP}$ that are part of $\\mathbf{LP}_b$, the loss $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}}$ is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}} &= \\frac{\\partial}{\\partial \\mathbf{LP}_{i j}} \\left( - \\frac{{\\mathbf{y}_b}_1 + {\\mathbf{y}_b}_2 + \\dots + {\\mathbf{y}_b}_m}{m} \\right)\\\\\n",
    "    &= 0 + 0 + \\dots + \\frac{\\partial}{\\partial \\mathbf{LP}_{i j}} \\left( - \\frac{\\mathbf{LP}_{i \\, j}}{m} \\right) + \\dots + 0 + 0\\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}} &= -\\frac{1}{m}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the elements of $\\mathbf{LP}$ that are *not* part of $\\mathbf{LP}_b$, the loss $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}_{i \\, j}}$ is $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175ad230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(batch_size), Yb] = -1 / batch_size\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5cca9",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `probs`.\n",
    "\n",
    "We want to find $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}}$, where $P$ is the tensor `probs`.\n",
    "\n",
    "Note that `logprobs` is defined as\n",
    "\n",
    "```python\n",
    "logprobs = probs.log()\n",
    "```\n",
    "\n",
    "So, instead of calculating $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}}$ directly, we can use the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}} \\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}}\n",
    "$$\n",
    "\n",
    "We already know $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}}$, so we just need to calculate $\\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}}$.\n",
    "\n",
    "If $\\mathbf{LP}$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{LP} = \\begin{bmatrix}\n",
    "lp_{1 \\, 1} & lp_{1 \\, 2} & \\dots & lp_{1 \\, n}\\\\\n",
    "lp_{2 \\, 1} & lp_{2 \\, 2} & \\dots & lp_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "lp_{m \\, 1} & lp_{m \\, 2} & \\dots & lp_{m \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $\\mathbf{P}$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\begin{bmatrix}\n",
    "p_{1 \\, 1} & p_{1 \\, 2} & \\dots & p_{1 \\, n}\\\\\n",
    "p_{2 \\, 1} & p_{2 \\, 2} & \\dots & p_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "p_{m \\, 1} & p_{m \\, 2} & \\dots & p_{m \\, n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then $\\mathbf{LP}$ can also be defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{LP} = \\log(\\mathbf{P}) = \\log\\left(\\begin{bmatrix}\n",
    "p_{1 \\, 1} & p_{1 \\, 2} & \\dots & p_{1 \\, n}\\\\\n",
    "p_{2 \\, 1} & p_{2 \\, 2} & \\dots & p_{2 \\, n}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "p_{m \\, 1} & p_{m \\, 2} & \\dots & p_{m \\, n}\\\\\n",
    "\\end{bmatrix}\\right) = \\begin{bmatrix}\n",
    "\\log(p_{1 \\, 1}) & \\log(p_{1 \\, 2}) & \\dots & \\log(p_{1 \\, n})\\\\\n",
    "\\log(p_{2 \\, 1}) & \\log(p_{2 \\, 2}) & \\dots & \\log(p_{2 \\, n})\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "\\log(p_{m \\, 1}) & \\log(p_{m \\, 2}) & \\dots & \\log(p_{m \\, n})\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So we know that $\\mathbf{LP}_{i \\, j} = \\log(\\mathbf{P}_{i \\, j})$. This means\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{LP}_{i\\,j}}{\\partial \\mathbf{P}_{i\\,j}} = \\frac{\\partial}{\\partial \\mathbf{P}_{i\\,j}} \\left( \\log(\\mathbf{P}_{i \\, j}) \\right) = \\frac{1}{\\mathbf{P}_{i \\, j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}} = \\begin{bmatrix}\n",
    "\\frac{1}{p_{1 \\, 1}} & \\frac{1}{p_{1 \\, 2}} & \\dots & \\frac{1}{p_{1 \\, n}}\\\\\n",
    "\\frac{1}{p_{2 \\, 1}} & \\frac{1}{p_{2 \\, 2}} & \\dots & \\frac{1}{p_{2 \\, n}}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "\\frac{1}{p_{m \\, 1}} & \\frac{1}{p_{m \\, 2}} & \\dots & \\frac{1}{p_{m \\, n}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we can do element-wise multiplication between $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LP}}$ and $\\frac{\\partial \\mathbf{LP}}{\\partial \\mathbf{P}}$ to get $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dd8de82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dprobs = dlogprobs * probs.pow(-1)\n",
    "cmp('probs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19bebd",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `counts_sum_inv`\n",
    "\n",
    "Note that `probs` is defined as\n",
    "\n",
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "But the sizes of `counts` and `counts_sum_inv` are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b206c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts.shape=torch.Size([32, 27]), counts_sum_inv.shape=torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f'{counts.shape=}, {counts_sum_inv.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010b359",
   "metadata": {},
   "source": [
    "This means PyTorch broadcasts the multiplication like so:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} \\circ \\mathbf{CSI} = \\begin{bmatrix}\n",
    "c_{1 \\: 1} & c_{1 \\: 2} & \\dots & c_{1 \\: 27}\\\\\n",
    "c_{2 \\: 1} & c_{2 \\: 2} & \\dots & c_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "c_{32 \\: 1} & c_{32 \\: 2} & \\dots & c_{32 \\: 27}\\\\\n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "csi_{1}\\\\\n",
    "csi_{2}\\\\\n",
    "\\vdots\\\\\n",
    "csi_{32}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "c_{1 \\: 1} & c_{1 \\: 2} & \\dots & c_{1 \\: 27}\\\\\n",
    "c_{2 \\: 1} & c_{2 \\: 2} & \\dots & c_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "c_{32 \\: 1} & c_{32 \\: 2} & \\dots & c_{32 \\: 27}\\\\\n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "csi_{1} & csi_{1} & \\dots & csi_{1}\\\\\n",
    "csi_{2} & csi_{2} & \\dots & csi_{2}\\\\\n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\\n",
    "csi_{32} & csi_{32} & \\dots & csi_{32}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "c_{1 \\: 1} \\: csi_{1} & c_{1 \\: 2} \\: csi_{1} & \\dots & c_{1 \\: 27} \\: csi_{1}\\\\\n",
    "c_{2 \\: 1} \\: csi_{2} & c_{2 \\: 2} \\: csi_{2} & \\dots & c_{2 \\: 27} \\: csi_{2}\\\\\n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\\n",
    "c_{32 \\: 1} \\: csi_{32} & c_{32 \\: 2} \\: csi_{32} & \\dots & c_{32 \\: 27} \\: csi_{32}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First let's call this broadcasted tensor $\\mathbf{CSI}'$ and calculate its partial derivative.\n",
    "\n",
    "$$\n",
    "\\mathbf{CSI}' = \\begin{bmatrix}\n",
    "csi_{1} & csi_{1} & \\dots & csi_{1}\\\\\n",
    "csi_{2} & csi_{2} & \\dots & csi_{2}\\\\\n",
    "\\vdots  & \\vdots & \\ddots & \\vdots\\\\\n",
    "csi_{32} & csi_{32} & \\dots & csi_{32}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CSI}'} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}} \\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{CSI}'}\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{P} = \\mathbf{C} \\circ \\mathbf{CSI}'$, $\\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{CSI}'}$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{CSI}'} = \\frac{\\partial}{\\partial \\mathbf{CSI}'} \\left( \\mathbf{C} \\circ \\mathbf{CSI}' \\right) = \\mathbf{C}\n",
    "$$\n",
    "\n",
    "Since the gradient of the vector $\\mathbf{CSI}$ is used multiple times (each column in the broadcast), we must sum each use of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92790ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cea05c",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `counts_sum`\n",
    "\n",
    "Note that `counts_sum_inv` is defined as\n",
    "\n",
    "```python\n",
    "counts_sum_inv = counts_sum**-1\n",
    "```\n",
    "\n",
    "Denoting `counts_sum_inv` as $\\mathbf{CSI}$ and `counts_sum` as $\\mathbf{CS}$\n",
    "\n",
    "$$\n",
    "\\mathbf{CSI} = \\mathbf{CS}^{-1}\n",
    "$$\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CS}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CSI}} \\frac{\\partial \\mathbf{CSI}}{\\partial \\mathbf{CS}}\n",
    "$$\n",
    "\n",
    "All we need to compute is $\\frac{\\partial \\mathbf{CSI}}{\\partial \\mathbf{CS}}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{CSI}}{\\partial \\mathbf{CS}} = \\frac{\\partial}{\\partial \\mathbf{CS}} \\left( \\mathbf{CS}^{-1} \\right) = - \\mathbf{CS}^{-2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13c5272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts_sum = dcounts_sum_inv * - counts_sum.pow(-2)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb685980",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `counts`\n",
    "\n",
    "Note that `probs` is defined as\n",
    "\n",
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```\n",
    "\n",
    "and `counts_sum` is defined as\n",
    "\n",
    "```python\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "```\n",
    "\n",
    "Representing `probs` as $\\mathbf{P}$, `counts` as $\\mathbf{C}$, `counts_sum` as $\\mathbf{CS}$, and `counts_sum_inv` as $\\mathbf{CSI}$, we get two equations involving $\\mathbf{C}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{P} = \\mathbf{C} * \\mathbf{CSI}\\\\\n",
    "    \\mathbf{CS} = \\sum_{i} \\mathbf{C}_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To calculate $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}}$, we must calculate the gradient for both of the above equations and add their contributions to the gradient of $\\mathbf{C}$.\n",
    "\n",
    "1. Remember, the sizes of `counts` and `counts_sum_inv` are different, causing `counts_sum_inv` to be broadcasted.\n",
    "\n",
    "    Using the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{P}} \\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{C}}\n",
    "    $$\n",
    "\n",
    "    All we need to compute is $\\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{C}}$, denoting the broadcasted tensor of $\\mathbf{CSI}$ as $\\mathbf{CSI}'$.\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathbf{P}}{\\partial \\mathbf{C}} = \\frac{\\partial}{\\partial \\mathbf{C}} \\left( \\mathbf{C} \\circ \\mathbf{CSI}' \\right) = \\mathbf{CSI}'\n",
    "    $$\n",
    "\n",
    "2. Defining $\\mathbf{C}$ as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{C} = \\begin{bmatrix}\n",
    "    c_{1 \\: 1} & c_{1 \\: 2} & \\dots & c_{1 \\: 27}\\\\\n",
    "    c_{2 \\: 1} & c_{2 \\: 2} & \\dots & c_{2 \\: 27}\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    c_{32 \\: 1} & c_{32 \\: 2} & \\dots & c_{32 \\: 27}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    we know that $\\mathbf{CS}$ is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{CS} = \\begin{bmatrix}\n",
    "    c_{1 \\: 1} + c_{1 \\: 2} + \\dots + c_{1 \\: 27}\\\\\n",
    "    c_{2 \\: 1} + c_{2 \\: 2} + \\dots + c_{2 \\: 27}\\\\\n",
    "    \\vdots\\\\\n",
    "    c_{32 \\: 1} + c_{32 \\: 2} + \\dots + c_{32 \\: 27}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    Using the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{CS}} \\frac{\\partial \\mathbf{CS}}{\\partial \\mathbf{C}}\n",
    "    $$\n",
    "\n",
    "    We only need to calculate $\\frac{\\partial \\mathbf{CS}}{\\partial \\mathbf{C}}$.\n",
    "\n",
    "    Since each element $c_{i j}$ of $\\mathbf{C}$ participates in a sum in some element of $\\mathbf{CS}$, the partial derivative $\\frac{\\partial \\mathbf{CS}_i}{\\partial \\mathbf{C}_{i j}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathbf{CS}_i}{\\partial \\mathbf{C}_{i j}} = \\frac{\\partial}{\\partial \\mathbf{C}_{i j}} \\left( c_{i \\, 1} + c_{i \\, 2} + \\dots + c_{i j} + \\dots c_{i \\, 27} \\right) = 1\n",
    "    $$\n",
    "\n",
    "    So, the partial derivative $\\frac{\\partial \\mathbf{CS}}{\\partial \\mathbf{C}}$ is a matrix the shape of $\\mathbf{C}$ with all elements being $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2f68d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dcounts = (counts_sum_inv * dprobs) + (torch.ones_like(counts) * dcounts_sum)\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a3bb1",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `norm_logits`\n",
    "\n",
    "Note that `counts` is defined as\n",
    "\n",
    "```python\n",
    "counts = norm_logits.exp()\n",
    "```\n",
    "\n",
    "Representing `counts` as $\\mathbf{C}$ and `norm_logits` as $\\mathbf{NL}$, we have:\n",
    "\n",
    "$$\n",
    "\\mathbf{NL} = \\begin{bmatrix}\n",
    "nl_{1 \\: 1} & nl_{1 \\: 2} & \\dots & nl_{1 \\: 27}\\\\\n",
    "nl_{2 \\: 1} & nl_{2 \\: 2} & \\dots & nl_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "nl_{32 \\: 1} & nl_{32 \\: 2} & \\dots & nl_{32 \\: 27}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This way, $\\mathbf{C}$ can be expressed as\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\exp(\\mathbf{NL}) = \\begin{bmatrix}\n",
    "e^{nl_{1 \\: 1}} & e^{nl_{1 \\: 2}} & \\dots & e^{nl_{1 \\: 27}}\\\\\n",
    "e^{nl_{2 \\: 1}} & e^{nl_{2 \\: 2}} & \\dots & e^{nl_{2 \\: 27}}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "e^{nl_{32 \\: 1}} & e^{nl_{32 \\: 2}} & \\dots & e^{nl_{32 \\: 27}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Using the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} \\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{NL}}\n",
    "$$\n",
    "\n",
    "For each partial derivative $\\frac{\\partial \\mathbf{C}_{ij}}{\\partial \\mathbf{NL}_{ij}}$, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{C}_{ij}}{\\partial \\mathbf{NL}_{ij}} = \\frac{\\partial}{\\partial \\mathbf{NL}_{ij}}\\left( e^{nl_{ij}} \\right) = e^{nl_{ij}}\n",
    "$$\n",
    "\n",
    "Therefore, $\\frac{\\partial \\mathbf{C}}{\\partial \\mathbf{NL}} = \\mathbf{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5632a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dnorm_logits = counts * dcounts\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d853c25",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `logits_maxes`\n",
    "\n",
    "Note that `norm_logits` is defined as\n",
    "\n",
    "```python\n",
    "norm_logits = logits - logits_maxes\n",
    "```\n",
    "\n",
    "Since `logits` and `logits_maxes` are not the same shape, the operation will be broadcasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6421b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([32, 27]), logit_maxes.shape=torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f'{logits.shape=}, {logit_maxes.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f2a1a",
   "metadata": {},
   "source": [
    "This means the vector `logit_maxes` will be repeated $27$ times. So `norm_logits` is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{NL} = \\begin{bmatrix}\n",
    "l_{1 \\: 1} - lm_1 & l_{1 \\: 2} - lm_1 & \\dots & l_{1 \\: 27} - lm_1\\\\\n",
    "l_{2 \\: 1} - lm_2 & l_{2 \\: 2} - lm_2 & \\dots & l_{2 \\: 27} - lm_2\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "l_{32 \\: 1} - lm_{32} & l_{32 \\: 2} - lm_{32} & \\dots & l_{32 \\: 27} - lm_{32}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The gradient $\\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}}$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}} = \\begin{bmatrix}\n",
    "-1\\\\\n",
    "-1\\\\\n",
    "\\vdots\\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Again using the chain rule, $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LM}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}} \\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}}$. Again, due to broadcasting, $\\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{LM}}$ is repeated. Since all its entries are $-1$, we can sum across $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}}$ and negate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "558f2fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogit_maxes = - dnorm_logits.sum(1, keepdim=True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4554c",
   "metadata": {},
   "source": [
    "Note that the logit normalization is performed to prevent floating point errors when values are too large. This means it should not have an effect on the loss, which means the gradient of `dlogit_maxes` should be approximately $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "891db07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7253e-09],\n",
       "        [-1.8626e-09],\n",
       "        [ 5.5879e-09],\n",
       "        [ 3.7253e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [-2.7940e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [ 3.7253e-09],\n",
       "        [-1.8626e-09],\n",
       "        [-2.7940e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [ 9.3132e-10],\n",
       "        [-0.0000e+00],\n",
       "        [ 1.8626e-09],\n",
       "        [-3.7253e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [ 9.3132e-10],\n",
       "        [-1.8626e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [-9.3132e-10],\n",
       "        [-0.0000e+00],\n",
       "        [ 2.7940e-09],\n",
       "        [-0.0000e+00],\n",
       "        [-9.3132e-10],\n",
       "        [-0.0000e+00],\n",
       "        [-9.3132e-10],\n",
       "        [-1.8626e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [-1.8626e-09]], device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed37c813",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `logits`\n",
    "\n",
    "The tensor `logits` is used twice, so its gradient is a sum of the two separate gradients.\n",
    "\n",
    "```python\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "```\n",
    "\n",
    "1. The `logits` tensor $\\mathbf{L}$ is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{L} = \\begin{bmatrix}\n",
    "    l_{1 \\: 1} & l_{1 \\: 2} & \\dots & l_{1 \\: 27}\\\\\n",
    "    l_{2 \\: 1} & l_{2 \\: 2} & \\dots & l_{2 \\: 27}\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    l_{32 \\: 1} & l_{32 \\: 2} & \\dots & l_{32 \\: 27}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The `logit_maxes` tensor is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{LM} = \\begin{bmatrix}\n",
    "    lm_1\\\\\n",
    "    lm_2\\\\\n",
    "    \\vdots\\\\\n",
    "    lm_{32}\\\\\n",
    "    \\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\max(l_{1 \\: 1}, l_{1 \\: 2}, \\dots, l_{1 \\: 27})\\\\\n",
    "    \\max(l_{2 \\: 1}, l_{2 \\: 2}, \\dots, l_{2 \\: 27})\\\\\n",
    "    \\vdots\\\\\n",
    "    \\max(l_{32 \\: 1}, l_{32 \\: 2}, \\dots, l_{32 \\: 27})\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The partial derivative $\\frac{\\partial \\mathbf{LM}_{i}}{\\partial \\mathbf{L}_{i j}}$ is $1$ if $l_{i j}$ is the maximum of the row $l_{i \\, 1}, l_{i \\, 2}, \\dots l_{i \\, j}, \\dots, l_{i \\, 27}$, and is $0$ otherwise. From here we can use the chain rule $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{LM}} \\frac{\\partial \\mathbf{LM}}{\\partial \\mathbf{L}}$\n",
    "\n",
    "2. The `norm_logits` tensor $\\mathbf{NL}$ is defined as\n",
    "\n",
    "    $$\n",
    "    \\mathbf{NL} = \\begin{bmatrix}\n",
    "    l_{1 \\: 1} - lm_1 & l_{1 \\: 2} - lm_1 & \\dots & l_{1 \\: 27} - lm_1\\\\\n",
    "    l_{2 \\: 1} - lm_2 & l_{2 \\: 2} - lm_2 & \\dots & l_{2 \\: 27} - lm_2\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    l_{32 \\: 1} - lm_{32} & l_{32 \\: 2} - lm_{32} & \\dots & l_{32 \\: 27} - lm_{32}\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    The partial derivative $\\frac{\\partial \\mathbf{NL}_{i j}}{\\partial \\mathbf{L}_{i j}}$ is $1$ because all $l_{i j}$ participate in the $\\mathbf{NL}$ matrix with coefficient $1$. This means\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{L}} = \\begin{bmatrix}\n",
    "    1 & 1 & \\dots & 1\\\\\n",
    "    1 & 1 & \\dots & 1\\\\\n",
    "    \\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "    1 & 1 & \\dots & 1\\\\\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    From here we can use the chain rule $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{NL}} \\frac{\\partial \\mathbf{NL}}{\\partial \\mathbf{L}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "986d420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogits = (F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes) + (dnorm_logits)\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd1867",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `h`, `W2`, and `b2`\n",
    "\n",
    "The `logits` tensor is defined as\n",
    "\n",
    "```python\n",
    "logits = h @ W2 + b2\n",
    "```\n",
    "\n",
    "Defining the $\\mathbf{H}$, $\\mathbf{W}_2$, and $\\mathbf{b}_2$ tensors as\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\begin{bmatrix}\n",
    "h_{1 \\: 1} & h_{1 \\: 2} & \\dots & h_{1 \\: 64}\\\\\n",
    "h_{2 \\: 1} & h_{2 \\: 2} & \\dots & h_{2 \\: 64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "h_{32 \\: 1} & h_{32 \\: 2} & \\dots & h_{32 \\: 64}\\\\\n",
    "\\end{bmatrix} \\quad\\quad \\mathbf{W}_2 = \\begin{bmatrix}\n",
    "w_{1 \\: 1} & w_{1 \\: 2} & \\dots & w_{1 \\: 27}\\\\\n",
    "w_{2 \\: 1} & w_{2 \\: 2} & \\dots & w_{2 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "w_{64 \\: 1} & w_{64 \\: 2} & \\dots & w_{64 \\: 27}\\\\\n",
    "\\end{bmatrix} \\quad\\quad \\mathbf{b}_2 = \\begin{bmatrix}\n",
    "b_{1} & b_{2} & \\dots & b_{27}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The product $\\mathbf{H} \\mathbf{W}_2$ is performed as\n",
    "\n",
    "$$\n",
    "\\mathbf{H} \\mathbf{W}_2 = \\begin{bmatrix}\n",
    "h_{1 \\: 1} \\, w_{1 \\: 1} + h_{1 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 1} & h_{1 \\: 1} \\, w_{1 \\: 2} + h_{1 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 2} & \\dots & h_{1 \\: 1} \\, w_{1 \\: 27} + h_{1 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 27}\\\\\n",
    "h_{2 \\: 1} \\, w_{1 \\: 1} + h_{2 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 1} & h_{2 \\: 1} \\, w_{1 \\: 2} + h_{2 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 2} & \\dots & h_{2 \\: 1} \\, w_{1 \\: 27} + h_{2 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "h_{32 \\: 1} \\, w_{1 \\: 1} + h_{32 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 1} & h_{32 \\: 1} \\, w_{1 \\: 2} + h_{32 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 2} & \\dots & h_{32 \\: 1} \\, w_{1 \\: 27} + h_{32 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 27}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the full expression $\\mathbf{L} = \\mathbf{H} \\mathbf{W}_2 + \\mathbf{b}_2$, which causes $\\mathbf{b}_2$ to be broadcasted, is\n",
    "\n",
    "$$\n",
    "\\mathbf{L} = \\mathbf{H} \\mathbf{W}_2 + \\mathbf{b}_2 = \\begin{bmatrix}\n",
    "h_{1 \\: 1} \\, w_{1 \\: 1} + h_{1 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 1} + b_1 & h_{1 \\: 1} \\, w_{1 \\: 2} + h_{1 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 2} + b_2 & \\dots & h_{1 \\: 1} \\, w_{1 \\: 27} + h_{1 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{1 \\: 64} \\, w_{64 \\: 27} + b_{27}\\\\\n",
    "h_{2 \\: 1} \\, w_{1 \\: 1} + h_{2 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 1} + b_1 & h_{2 \\: 1} \\, w_{1 \\: 2} + h_{2 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 2} + b_2 & \\dots & h_{2 \\: 1} \\, w_{1 \\: 27} + h_{2 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{2 \\: 64} \\, w_{64 \\: 27} + b_{27}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "h_{32 \\: 1} \\, w_{1 \\: 1} + h_{32 \\: 2} \\, w_{2 \\: 1} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 1} + b_{1} & h_{32 \\: 1} \\, w_{1 \\: 2} + h_{32 \\: 2} \\, w_{2 \\: 2} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 2} + b_{2} & \\dots & h_{32 \\: 1} \\, w_{1 \\: 27} + h_{32 \\: 2} \\, w_{2 \\: 27} + \\dots + h_{32 \\: 64} \\, w_{64 \\: 27} + b_{27}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For $\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}}$, the partial derivative for entry $i,j$, $\\frac{\\partial \\mathbf{L}_{i j}}{\\partial \\mathbf{H}_{i j}}$, is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}_{i j}}{\\partial \\mathbf{H}_{i j}} = \\frac{\\partial}{\\partial h_{i j}} \\left( h_{i \\: 1} \\, w_{1 \\: i} + h_{i \\: 2} \\, w_{2 \\: i} + \\dots + h_{i \\: j} \\, w_{j \\: i} + \\dots + h_{i \\: 64} \\, w_{64 \\: i} + b_j \\right) = w_{j \\: i}\n",
    "$$\n",
    "\n",
    "which means the partial derivative $\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}}$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}} = {\\mathbf{W}_2}^\\mathrm{T}\n",
    "$$\n",
    "\n",
    "By similar logic, the partial derivative $\\frac{\\partial \\mathbf{L}}{\\partial {\\mathbf{W}_2}}$, is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}}{\\partial {\\mathbf{W}_2}}\n",
    "$$\n",
    "\n",
    "Lastly, for $\\mathbf{b}_2$, the partial derivative $\\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{b}_2}$ for entry $i,j$, $\\frac{\\partial \\mathbf{L}_{i j}}{\\partial {\\mathbf{b}_2}_{i j}}$, is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{L}_{i j}}{\\partial {\\mathbf{b}_2}_{i j}} = \\frac{\\partial}{\\partial {b}_{j}} \\left( h_{i \\: 1} \\, w_{1 \\: i} + h_{i \\: 2} \\, w_{2 \\: i} + \\dots + h_{i \\: j} \\, w_{j \\: i} + \\dots + h_{i \\: 64} \\, w_{64 \\: i} + b_j \\right) = 1\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{b}_2$ is broadcasted, there are $64$ contributions to the gradient $\\frac{\\partial \\mathbf{L}}{\\partial {\\mathbf{b}_2}}$ (one for each row).\n",
    "\n",
    "From here, we can use the chain rule to calculate the derivatives of $\\mathbf{L}$ with respect to $\\mathbf{H}$, $\\mathbf{W}_2$, and $\\mathbf{b}_2$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{H}}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{W}_2} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{W}_2}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{b}_2} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{b}_2}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f93c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowing the shapes of the tensors involved in the matrix multiplication can help you figure out the correct way to multiply the gradients.\n",
      "logits.shape=torch.Size([32, 27]), W2.shape=torch.Size([64, 27]), h.shape=torch.Size([32, 64]), b2.shape=torch.Size([27])\n"
     ]
    }
   ],
   "source": [
    "print('Knowing the shapes of the tensors involved in the matrix multiplication can help you figure out the correct way to multiply the gradients.')\n",
    "print(f'{logits.shape=}, {W2.shape=}, {h.shape=}, {b2.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7433241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0, keepdim=True)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf08476",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `hpreact`\n",
    "\n",
    "The tensor `h` is defined as\n",
    "\n",
    "```python\n",
    "h = torch.tanh(hpreact)\n",
    "```\n",
    "\n",
    "Note the derivative of $\\mathrm{tanh}$\n",
    "\n",
    "$$\n",
    "y = \\mathrm{tanh}(x) \\Rightarrow \\frac{dy}{dx} = 1 - y^2\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{H} = \\mathrm{tanh}(\\mathbf{H}_{pre \\, activation})$, we know that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{H}}{\\partial \\mathbf{H}_{pre \\,activation}} = 1 - \\mathbf{H}^2\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}} \\frac{\\partial \\mathbf{H}}{\\partial \\mathbf{H}_{pre \\, activation}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e2dea48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n"
     ]
    }
   ],
   "source": [
    "dhpreact = dh * (1.0 - h.pow(2))\n",
    "cmp('hpreact', dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6936812",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bngain`, `bnraw`, and `bnbias`\n",
    "\n",
    "The tensor `hpreact` is defined as\n",
    "\n",
    "```python\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```\n",
    "\n",
    "Note the shapes of each tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8716e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact.shape=torch.Size([32, 64]), bngain.shape=torch.Size([1, 64]), bnraw.shape=torch.Size([32, 64]), bnbias.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{hpreact.shape=}, {bngain.shape=}, {bnraw.shape=}, {bnbias.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fcb1d",
   "metadata": {},
   "source": [
    "Defining the ${\\mathbf{b}_{n}}_{gain}$, ${\\mathbf{B}_{n}}_{raw}$, and ${\\mathbf{b}_{n}}_{bias}$ tensors as\n",
    "\n",
    "$$\n",
    "{\\mathbf{b}_{n}}_{gain} = \\begin{bmatrix}\n",
    "{{b_n}_{gain}}_{1} & {{b_n}_{gain}}_{2} & \\dots & {{b_n}_{gain}}_{64}\n",
    "\\end{bmatrix} \\quad\\quad {\\mathbf{B}_{n}}_{raw} = \\begin{bmatrix}\n",
    "{{b_n}_{raw}}_{1 \\: 1} & {{b_n}_{raw}}_{1 \\: 2} & \\dots & {{b_n}_{raw}}_{1 \\: 64}\\\\\n",
    "{{b_n}_{raw}}_{2 \\: 1} & {{b_n}_{raw}}_{2 \\: 2} & \\dots & {{b_n}_{raw}}_{2 \\: 64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "{{b_n}_{raw}}_{32 \\: 1} & {{b_n}_{raw}}_{32 \\: 2} & \\dots & {{b_n}_{raw}}_{32 \\: 64}\\\\\n",
    "\\end{bmatrix} \\quad\\quad {\\mathbf{b}_{n}}_{bias} = \\begin{bmatrix}\n",
    "{{b_n}_{bias}}_{1} & {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{bias}}_{64}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The full operation $\\mathbf{H}_{pre \\, activation} = {\\mathbf{b}_{n}}_{gain} \\circ {\\mathbf{B}_{n}}_{raw} + {\\mathbf{b}_{n}}_{bias}$, including broadcasting, is\n",
    "\n",
    "$$\n",
    "\\mathbf{H}_{pre \\, activation} = {\\mathbf{b}_{n}}_{gain} \\circ {\\mathbf{B}_{n}}_{raw} + {\\mathbf{b}_{n}}_{bias} = \\begin{bmatrix}\n",
    "{{b_n}_{gain}}_{1} \\: {{b_n}_{raw}}_{1 \\: 1} + {{b_n}_{bias}}_{1} & {{b_n}_{gain}}_{2} \\: {{b_n}_{raw}}_{1 \\: 2} + {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{gain}}_{64} \\: {{b_n}_{raw}}_{1 \\: 64} + {{b_n}_{bias}}_{64}\\\\\n",
    "{{b_n}_{gain}}_{1} \\: {{b_n}_{raw}}_{2 \\: 1} + {{b_n}_{bias}}_{1} & {{b_n}_{gain}}_{2} \\: {{b_n}_{raw}}_{2 \\: 2} + {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{gain}}_{64} \\: {{b_n}_{raw}}_{2 \\: 64} + {{b_n}_{bias}}_{64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "{{b_n}_{gain}}_{1} \\: {{b_n}_{raw}}_{32 \\: 1} + {{b_n}_{bias}}_{1} & {{b_n}_{gain}}_{2} \\: {{b_n}_{raw}}_{32 \\: 2} + {{b_n}_{bias}}_{2} & \\dots & {{b_n}_{gain}}_{64} \\: {{b_n}_{raw}}_{32 \\: 64} + {{b_n}_{bias}}_{64}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculating the derivative $\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{gain}}}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}_{i j}}{\\partial {{\\mathbf{b}_{n}}_{gain}}_{i j}} = \\frac{\\partial}{\\partial {{{b}_{n}}_{gain}}_{j}} \\left( {{b_n}_{gain}}_{j} \\: {{b_n}_{raw}}_{i \\: j} + {{b_n}_{bias}}_{j} \\right) = {{b_n}_{raw}}_{i \\: j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{gain}}} = {\\mathbf{B}_{n}}_{raw}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Calculating the derivative $\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{B}_{n}}_{raw}}}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}_{i j}}{\\partial {{\\mathbf{B}_{n}}_{raw}}_{i j}} = \\frac{\\partial}{\\partial {{{b}_{n}}_{gain}}_{j}} \\left( {{b_n}_{gain}}_{j} \\: {{b_n}_{raw}}_{i \\: j} + {{b_n}_{bias}}_{j} \\right) = {{b_n}_{gain}}_{j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{B}_{n}}_{raw}}} = {\\mathbf{b}_{n}}_{gain}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Calculating the derivative $\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{bias}}}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}_{i j}}{\\partial {{\\mathbf{b}_{n}}_{bias}}_{i j}} = \\frac{\\partial}{\\partial {{{b}_{n}}_{gain}}_{j}} \\left( {{b_n}_{gain}}_{j} \\: {{b_n}_{raw}}_{i \\: j} + {{b_n}_{bias}}_{j} \\right) = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre \\, activation}}}{\\partial {{\\mathbf{b}_{n}}_{bias}}} = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{b}_{n}}_{gain}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} \\frac{\\partial \\mathbf{H}_{pre \\, activation}}{\\partial {\\mathbf{b}_{n}}_{gain}}\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_{n}}_{raw}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} \\frac{\\partial \\mathbf{H}_{pre \\, activation}}{\\partial {\\mathbf{B}_{n}}_{raw}}\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{b}_{n}}_{bias}} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{H}_{pre \\, activation}} \\frac{\\partial \\mathbf{H}_{pre \\, activation}}{\\partial {\\mathbf{b}_{n}}_{bias}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1948637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "dbnraw = dhpreact * bngain\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnbias', dbnbias, bnbias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba1534",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bnvar_inv`\n",
    "\n",
    "The tensor `bnraw` is defined as\n",
    "\n",
    "```python\n",
    "bnraw = bndiff * bnvar_inv\n",
    "```\n",
    "\n",
    "Note the shapes are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85f1d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnraw.shape=torch.Size([32, 64]), bndiff.shape=torch.Size([32, 64]), bnvar_inv.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{bnraw.shape=}, {bndiff.shape=}, {bnvar_inv.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ff900",
   "metadata": {},
   "source": [
    "So `bnvar_inv` will be broadcasted.\n",
    "\n",
    "We know how to do back-propagation for element-wise multiplication: we just multiply ${\\mathbf{B}_n}_{diff}$ by $\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{raw}}$ and sum across the rows (because of broadcasting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c655d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n"
     ]
    }
   ],
   "source": [
    "dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim=True)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3bc3d",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bnvar`\n",
    "\n",
    "The tensor `bnvar_inv` is defined as\n",
    "\n",
    "```python\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "```\n",
    "\n",
    "Taking the derivative \n",
    "\n",
    "$$\n",
    "\\frac{\\partial {{\\mathbf{B}_n}_{var}}_{inv}}{\\partial {\\mathbf{B}_n}_{var}} = \\frac{\\partial}{\\partial {\\mathbf{B}_n}_{var}} \\left( \\left( {\\mathbf{B}_n}_{var} + 1 \\times 10^{-5} \\right)^{-\\frac{1}{2}} \\right) = - \\frac{1}{2} \\left( {\\mathbf{B}_n}_{var} + 1 \\times 10^{-5} \\right)^{-\\frac{3}{2}}\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{var}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{B}_n}_{var}}_{inv}} \\frac{\\partial {{\\mathbf{B}_n}_{var}}_{inv}}{\\partial {\\mathbf{B}_n}_{var}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ef6ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar           | exact: False | approximate: True  | maxdiff: 5.820766091346741e-10\n"
     ]
    }
   ],
   "source": [
    "dbnvar = dbnvar_inv * (-(1.0/2.0) * (bnvar + 1e-5)**(-3.0/2.0))\n",
    "cmp('bnvar', dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df660a",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bndiff2`\n",
    "\n",
    "The tensor `bnvar` is defined as\n",
    "\n",
    "```python\n",
    "bnvar = 1 / (batch_size-1) * bndiff2.sum(0, keepdim=True)\n",
    "```\n",
    "\n",
    "Let $m=32$ be the batch size. If we represent ${{\\mathbf{B}_n}_{diff}}_2$ as\n",
    "\n",
    "$$\n",
    "{{\\mathbf{B}_n}_{diff}}_2 = \\begin{bmatrix}\n",
    "{{{b_n}_{diff}}_2}_{\\: 1 \\, 1} & {{{b_n}_{diff}}_2}_{\\: 1 \\, 2} & \\dots & {{{b_n}_{diff}}_2}_{\\: 1 \\, 64}\\\\\n",
    "{{{b_n}_{diff}}_2}_{\\: 2 \\, 1} & {{{b_n}_{diff}}_2}_{\\: 2 \\, 2} & \\dots & {{{b_n}_{diff}}_2}_{\\: 2 \\, 64}\\\\\n",
    "\\vdots   & \\vdots   & \\ddots & \\vdots\\\\\n",
    "{{{b_n}_{diff}}_2}_{\\: 32 \\, 1} & {{{b_n}_{diff}}_2}_{\\: 32 \\, 2} & \\dots & {{{b_n}_{diff}}_2}_{\\: 32 \\, 64}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we define ${\\mathbf{b}_n}_{var}$ as\n",
    "\n",
    "$$\n",
    "{\\mathbf{b}_n}_{var} = \\begin{bmatrix}\n",
    "\\frac{1}{m-1} \\left( {{{b_n}_{diff}}_2}_{\\: 1 \\, 1} + {{{b_n}_{diff}}_2}_{\\: 2 \\, 1} + \\dots + {{{b_n}_{diff}}_2}_{\\: 32 \\, 1} \\right) & \\frac{1}{m-1} \\left( {{{b_n}_{diff}}_2}_{\\: 1 \\, 2} + {{{b_n}_{diff}}_2}_{\\: 2 \\, 2} + \\dots + {{{b_n}_{diff}}_2}_{\\: 32 \\, 2} \\right) & \\dots & \\frac{1}{m-1} \\left( {{{b_n}_{diff}}_2}_{\\: 1 \\, 64} + {{{b_n}_{diff}}_2}_{\\: 2 \\, 64} + \\dots + {{{b_n}_{diff}}_2}_{\\: 32 \\, 64} \\right)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Taking the partial derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial {{\\mathbf{b}_n}_{var}}_{i j}}{\\partial {{{\\mathbf{B}_n}_{diff}}_2}_{i j}} = \\frac{\\partial}{\\partial {{{{b}_n}_{diff}}_2}_{i j}} \\left( \\frac{1}{m-1} \\left( {{{b_n}_{diff}}_2}_{\\: 1 \\, j} + {{{b_n}_{diff}}_2}_{\\: 2 \\, j} + \\dots + {{{b_n}_{diff}}_2}_{\\: i \\, j} + \\dots + {{{b_n}_{diff}}_2}_{\\: 32 \\, j} \\right) \\right) = \\frac{1}{m-1}\n",
    "$$\n",
    "\n",
    "From here we can use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial {{{\\mathbf{B}_n}_{diff}}_2}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{b}_n}_{var}}} \\frac{\\partial {{\\mathbf{b}_n}_{var}}}{\\partial {{{\\mathbf{B}_n}_{diff}}_2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc01e450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff2         | exact: False | approximate: True  | maxdiff: 1.8189894035458565e-11\n"
     ]
    }
   ],
   "source": [
    "dbndiff2 = dbnvar * ((1.0 / (batch_size - 1)) * torch.ones_like(bndiff2))\n",
    "cmp('bndiff2', dbndiff2, bndiff2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8f2d7",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bndiff`\n",
    "\n",
    "The tensor `bndiff` is used twice, once in the definition of `bnraw` and again in the definition of `bndiff2`.\n",
    "\n",
    "```python\n",
    "bnraw = bndiff * bnvar_inv\n",
    "bndiff2 = bndiff**2\n",
    "```\n",
    "\n",
    "1. The partial derivative $\\frac{\\partial {\\mathbf{B}_n}_{raw}}{\\partial {\\mathbf{B}_n}_{diff}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial {\\mathbf{B}_n}_{raw}}{\\partial {\\mathbf{B}_n}_{diff}} = {{\\mathbf{B}_n}_{var}}_{inv}\n",
    "    $$\n",
    "\n",
    "    From here we can use the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{diff}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{b}_n}_{raw}}} \\frac{\\partial {{\\mathbf{b}_n}_{raw}}}{\\partial {\\mathbf{B}_n}_{diff}}\n",
    "    $$\n",
    "\n",
    "    Note that this will be broadcasted because $\\frac{\\partial {\\mathbf{B}_n}_{raw}}{\\partial {\\mathbf{B}_n}_{diff}} = {{\\mathbf{B}_n}_{var}}_{inv}$ is a vector and $\\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{b}_n}_{raw}}}$ is a 2D matrix.\n",
    "\n",
    "2. The partial derivative $\\frac{\\partial {{\\mathbf{B}_n}_{diff}}_2}{\\partial {\\mathbf{B}_n}_{diff}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial {{\\mathbf{B}_n}_{diff}}_2}{\\partial {\\mathbf{B}_n}_{diff}} = \\frac{\\partial}{\\partial {\\mathbf{B}_n}_{diff}} \\left( {{\\mathbf{B}_n}_{diff}}^2 \\right) = 2 {{\\mathbf{B}_n}_{diff}}\n",
    "    $$\n",
    "\n",
    "    From here we can use the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{diff}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{B}_n}_{diff}}_2} \\frac{\\partial {{\\mathbf{B}_n}_{diff}}_2}{\\partial {\\mathbf{B}_n}_{diff}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78f46a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting the shapes of the tensors can also help\n",
      "bnraw.shape=torch.Size([32, 64]), bndiff.shape=torch.Size([32, 64]), bndiff2.shape=torch.Size([32, 64]), bnvar_inv.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print('Inspecting the shapes of the tensors can also help')\n",
    "print(f'{bnraw.shape=}, {bndiff.shape=}, {bndiff2.shape=}, {bnvar_inv.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b71f62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbndiff         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "dbndiff = ( dbnraw * bnvar_inv ) + ( dbndiff2 * (2 * bndiff) )\n",
    "cmp('dbndiff', dbndiff, bndiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde5326",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `bnmeani`\n",
    "\n",
    "The tensor `bndiff` is defined as\n",
    "\n",
    "```python\n",
    "bndiff = hprebn - bnmeani\n",
    "```\n",
    "\n",
    "Note that `bndiff` and `hprebn` are  different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a27dcdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff.shape=torch.Size([32, 64]), bnmeani.shape=torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{bndiff.shape=}, {bnmeani.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b75142b",
   "metadata": {},
   "source": [
    "So `bnmeani` will be broadcasted.\n",
    "\n",
    "Since this operation is a subtraction, the partial derivative $\\frac{\\partial {\\mathbf{B}_n}_{diff}}{\\partial {{\\mathbf{b}_n}_{mean}}_i}$ is $-1$. Then we use chain rule $\\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{b}_n}_{mean}}_i} = \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{B}_n}_{diff}} \\frac{\\partial {\\mathbf{B}_n}_{diff}}{\\partial {{\\mathbf{b}_n}_{mean}}_i}$.\n",
    "\n",
    "Since `bnmeani` is broadcasted, we must perform a sum to add all the contributions to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca94e0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "dbnmeani = (dbndiff * (-1 * torch.ones_like(bndiff))).sum(0, keepdim=True)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33a358",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `hprebn`\n",
    "\n",
    "The tensor `hprebn` is used twice, once in the definition of `bndiff` and again in the definition of `bnmeani`.\n",
    "\n",
    "```python\n",
    "bndiff = hprebn - bnmeani\n",
    "bnmeani = 1.0 / batch_size * hprebn.sum(0, keepdim=True)\n",
    "```\n",
    "\n",
    "Note that `bndiff` and `hprebn` are the same sizes.\n",
    "\n",
    "1. The partial derivative $\\frac{\\partial {\\mathbf{B}_n}_{diff}}{\\partial {\\mathbf{H}_{pre}}_{b_n}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial {\\mathbf{B}_n}_{diff}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} = 1\n",
    "    $$\n",
    "\n",
    "    From here we can use the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{B}_n}_{diff}}} \\frac{\\partial {{\\mathbf{B}_n}_{diff}}}{\\partial {\\mathbf{H}_{pre}}_{b_n}}\n",
    "    $$\n",
    "\n",
    "2. The partial derivative $\\frac{\\partial {{\\mathbf{B}_n}_{mean}}_i}{\\partial {\\mathbf{H}_{pre}}_{b_n}}$ is\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial {{\\mathbf{B}_n}_{mean}}_i}{\\partial {\\mathbf{H}_{pre}}_{b_n}} = \\frac{1}{m}\n",
    "    $$\n",
    "\n",
    "    where $m$ is the batch size. From here we can use the chain rule\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} = \\frac{\\partial \\mathscr{L}}{\\partial {{\\mathbf{B}_n}_{mean}}_i} \\frac{\\partial {{\\mathbf{B}_n}_{mean}}_i}{\\partial {\\mathbf{H}_{pre}}_{b_n}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf3da1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "dhprebn = ( dbndiff.clone() ) + ( dbnmeani * ((1.0 / batch_size) * torch.ones_like(hprebn)) )\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae0b0c",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `embcat`, `W1`, and `b1`\n",
    "\n",
    "The `hprebn` tensor is defined as\n",
    "\n",
    "```python\n",
    "hprebn = embcat @ W1 + b1\n",
    "```\n",
    "\n",
    "Using the same logic as the matrix multiplication expression `logits = h @ W2 + b2`, we know that the gradients are\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{C}_{cat}} &= {\\mathbf{W}_1}^T\\\\\n",
    "\\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{W}_1} &= {\\mathbf{C}_{cat}}^T\\\\\n",
    "\\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{b}_1} &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{C}_{cat}$ is `embcat`, $\\mathbf{W}_1$ is `W1`, and $\\mathbf{b}_1$ is `b1`. Note that `b1` is broadcasted across the 0th dimension (down columns) due to its differing shape from `embcat @ W1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f5e48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn.shape=torch.Size([32, 64]), embcat.shape=torch.Size([32, 30]), W1.shape=torch.Size([30, 64]), b1.shape=torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(f'{hprebn.shape=}, {embcat.shape=}, {W1.shape=}, {b1.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc750c7",
   "metadata": {},
   "source": [
    "From here, we can use the chain rule to calculate the partial derivatives of $\\mathscr{L}$ with respect to $\\mathbf{C}_{cat}$, $\\mathbf{W}_1$, and $\\mathbf{b}_1$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}_{cat}} &= \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} \\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{C}_{cat}}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{W}_1} &= \\frac{\\partial \\mathscr{L}}{\\partial {\\mathbf{H}_{pre}}_{b_n}} \\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{W}_1}\\\\[10pt]\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{b}_1} &= \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{L}} \\frac{\\partial {\\mathbf{H}_{pre}}_{b_n}}{\\partial \\mathbf{b}_1}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "485e8b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 4.889443516731262e-09\n"
     ]
    }
   ],
   "source": [
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0, keepdim=True)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68809dfb",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `emb`\n",
    "\n",
    "Note that `embcat` is jut a concatenation of `emb`, which in PyTorch is just a different view of the tensor.\n",
    "\n",
    "```python\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b26b8272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embcat.shape=torch.Size([32, 30]), emb.shape=torch.Size([32, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f'{embcat.shape=}, {emb.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa44d33",
   "metadata": {},
   "source": [
    "For the backward pass, we just undo this operation by viewing it as the shape of `emb`. Once we deal with the shape of the tensor, and knowing that $\\frac{\\partial \\mathbf{C}_{cat}}{\\partial \\mathbf{C}} = 1$, we can then use chain rule to get $\\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}} = \\frac{\\partial \\mathscr{L}}{\\partial \\mathbf{C}_{cat}} \\frac{\\partial \\mathbf{C}_{cat}}{\\partial \\mathbf{C}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9d3a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n"
     ]
    }
   ],
   "source": [
    "demb = dembcat.view(emb.shape)\n",
    "cmp('emb', demb, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d6801",
   "metadata": {},
   "source": [
    "### Derivative of `loss` with respect to `C`\n",
    "\n",
    "Note that the tensor `emb` is defined as\n",
    "\n",
    "```python\n",
    "emb = C[Xb]\n",
    "```\n",
    "\n",
    "Note what is happening in the example below. The tensor `Xb`, which is $32 \\times 3$, is picking out $3$ rows from `C`, and is doing so $32$ times. For each of the three rows it selects, it gets a vector of length $10$. This is why `emb` is $32 \\times 3 \\times 10$: there are $32$ examples, each with $3$ letters, and each letter represented by a vector with $10$ elements.\n",
    "\n",
    "Note that this only works if all elements in `Xb` are greater than or equal to $0$ and less than $27$, otherwise it would not be able to index `C` (because it is a $27 \\times 10$ tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "110f2c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb.shape=torch.Size([32, 3, 10]), C.shape=torch.Size([27, 10]), Xb.shape=torch.Size([32, 3])\n",
      "tensor([ 1, 18,  9], device='cuda:0')\n",
      "tensor([[-1.3203e+00, -6.3178e-04,  4.4023e-01,  5.5836e-01, -1.7109e-01,\n",
      "          2.8308e-01,  4.6579e-01, -8.2765e-01, -4.9996e-01,  1.0358e+00],\n",
      "        [ 7.9658e-01, -4.8727e-01,  2.7846e-01,  1.9647e+00,  1.1010e-01,\n",
      "         -1.3208e+00, -1.4640e+00,  1.7635e-01, -2.2272e-01, -7.8665e-01],\n",
      "        [-9.8619e-01,  4.8606e-01,  4.8744e-01,  4.4705e-01,  2.1134e+00,\n",
      "          4.2250e-01, -3.1128e-03,  2.1297e+00, -3.8987e-02,  5.3950e-01]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([-1.3203e+00, -6.3178e-04,  4.4023e-01,  5.5836e-01, -1.7109e-01,\n",
      "         2.8308e-01,  4.6579e-01, -8.2765e-01, -4.9996e-01,  1.0358e+00],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([-1.9801, -0.4270,  0.2572,  0.0397,  0.1097, -1.6633,  1.4520, -1.8088,\n",
      "         0.1398, -0.5068], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([[-1.3203e+00, -6.3178e-04,  4.4023e-01,  5.5836e-01, -1.7109e-01,\n",
      "          2.8308e-01,  4.6579e-01, -8.2765e-01, -4.9996e-01,  1.0358e+00],\n",
      "        [-1.3203e+00, -6.3178e-04,  4.4023e-01,  5.5836e-01, -1.7109e-01,\n",
      "          2.8308e-01,  4.6579e-01, -8.2765e-01, -4.9996e-01,  1.0358e+00],\n",
      "        [-1.9801e+00, -4.2700e-01,  2.5717e-01,  3.9749e-02,  1.0970e-01,\n",
      "         -1.6633e+00,  1.4520e+00, -1.8088e+00,  1.3979e-01, -5.0680e-01]])\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n",
      "tensor(True, device='cuda:0')\n",
      "tensor(True, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f'{emb.shape=}, {C.shape=}, {Xb.shape=}')\n",
    "print(Xb[0])\n",
    "print(C[Xb[0]])\n",
    "print(C[1])\n",
    "print(C[4])\n",
    "print(torch.tensor([C[1].tolist(), C[1].tolist(), C[4].tolist()]))\n",
    "print(torch.tensor([C[1].tolist(), C[1].tolist(), C[4].tolist()]) == C[Xb[0].cpu()].cpu())\n",
    "print(torch.all(Xb >= 0))\n",
    "print(torch.all(Xb < 27))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b4aee",
   "metadata": {},
   "source": [
    "To back-propagate this operation, we need to \"route\" the gradient from `emb` to the indices of `C` that are chosen by `Xb`. The rest of the elements of `C` should have a gradient of 0.\n",
    "\n",
    "To do this, we initialize the gradient of `C` to all zeros and iterate through all the letters in `Xb`. For each letter we add the gradient from that letter in `emb` to the corresponding index in `C` (noting that the gradient is a vector of length $10$, since that is how letters are embedded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed748ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C               | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k, j]\n",
    "        dC[ix] += demb[k, j]\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b49347",
   "metadata": {},
   "source": [
    "## Exercise 2 - Back-Propagation Manually through the Entire Cross-Entropy Calculation\n",
    "\n",
    "We perform cross-entropy loss as such:\n",
    "\n",
    "```python\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(batch_size), Yb].mean()\n",
    "```\n",
    "\n",
    "For a single example, `logits` would be a $1 \\times 27$ vector $\\mathbf{\\ell}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{\\ell} = \\begin{bmatrix} \\ell_1 & \\ell_2 & \\dots & \\ell_{27} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get the counts vector $\\mathbf{c}$, we perform element-wise exponentiation.\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\begin{bmatrix} e^{\\ell_1} & e^{\\ell_2} & \\dots & e^{\\ell_{27}} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then to get probabilities, we divide each of the counts by the sum of the counts (softmax).\n",
    "\n",
    "$$\n",
    "\\mathbf{p} = \\begin{bmatrix} \\frac{e^{\\ell_1}}{\\sum_{j=1}^{27} e^{\\ell_j}} & \\frac{e^{\\ell_2}}{\\sum_{j=1}^{27} e^{\\ell_j}} & \\dots & \\frac{e^{\\ell_{27}}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We then take the negative logarithm to get negative log likelihoods (NLLs).\n",
    "\n",
    "$$\n",
    "\\mathbf{p'} = \\begin{bmatrix} -\\log\\left(\\frac{e^{\\ell_1}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) & -\\log\\left(\\frac{e^{\\ell_2}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) & \\dots & -\\log\\left(\\frac{e^{\\ell_{27}}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the loss of this example is the NLL of the correct label. Let's call this label $y$, so the correct logit is $\\ell_y$, and the expression for loss, $\\mathscr{L}$, becomes\n",
    "\n",
    "$$\n",
    "\\mathscr{L} = -\\log\\left(\\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right)\n",
    "$$\n",
    "\n",
    "We want to find an expression for loss with respect to each logit, meaning $\\frac{\\partial \\mathscr{L}}{\\partial \\ell_i}$ for all $i = 1, 2, \\dots, 27$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= \\frac{\\partial}{\\partial \\ell_i} \\left( -\\log\\left(\\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) \\right)\\\\\n",
    "        &= - \\frac{\\partial}{\\partial \\ell_i} \\left( \\log\\left(\\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\right) \\right)\\\\\n",
    "        &= - \\frac{1}{\\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}}} \\frac{\\partial}{\\partial \\ell_i} \\left( \\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "        &= - \\frac{\\sum_{j=1}^{27} e^{\\ell_j}}{e^{\\ell_y}} \\frac{\\partial}{\\partial \\ell_i} \\left( \\frac{e^{\\ell_y}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "        &= - \\frac{\\sum_{j=1}^{27} e^{\\ell_j}}{e^{\\ell_y}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_y} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} \\frac{\\partial}{\\partial \\ell_i} \\left( \\sum_{j=1}^{27} e^{\\ell_j} \\right)}{\\left( \\sum_{j=1}^{27} e^{\\ell_j} \\right)^2} \\right)\\\\\n",
    "        &= - \\frac{\\sum_{j=1}^{27} e^{\\ell_j}}{e^{\\ell_y}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_y} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} e^{\\ell_i}}{\\left( \\sum_{j=1}^{27} e^{\\ell_j} \\right)^2} \\right)\\\\\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_y} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We now have to separate cases:\n",
    "\n",
    "- If $i = y$:\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= - \\frac{1}{e^{\\ell_i}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_i} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_i} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{1}{e^{\\ell_i}} \\left( \\frac{e^{\\ell_i} \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_i} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{\\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\\\\n",
    "            &= \\frac{e^{\\ell_i} - \\sum_{j=1}^{27} e^{\\ell_j}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\\\\n",
    "            &= \\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} - \\frac{\\sum_{j=1}^{27} e^{\\ell_j}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\\\\n",
    "        \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= \\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} - 1\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "    Note that $\\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}}$ is the definition of $p_i$ (from the softmax probability vector $\\mathbf{p}$).\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} = p_i - 1\n",
    "    $$\n",
    "\n",
    "- If $i \\ne y$:\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{\\frac{\\partial}{\\partial \\ell_i} \\left( e^{\\ell_y} \\right) \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{0 \\cdot \\sum_{j=1}^{27} e^{\\ell_j} - e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{0 - e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= - \\frac{1}{e^{\\ell_y}} \\left( \\frac{- e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "            &= \\frac{1}{e^{\\ell_y}} \\left( \\frac{e^{\\ell_y} e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}} \\right)\\\\\n",
    "        \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} &= \\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}}\\\\\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "    Again, note that $\\frac{e^{\\ell_i}}{\\sum_{j=1}^{27} e^{\\ell_j}}$ is the definition of $p_i$ (from the softmax probability vector $\\mathbf{p}$).\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial \\mathscr{L}}{\\partial \\ell_i} = p_i\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e895d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 4.889443516731262e-09\n"
     ]
    }
   ],
   "source": [
    "dlogits = F.softmax(logits, 1)      # Each logit[i,j] starts with gradient p[i,j]\n",
    "dlogits[range(batch_size), Yb] -= 1 # Subtract 1 from the correct labels\n",
    "dlogits /= batch_size               # Divide by the batch size since loss is averaged over batches\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03685a0a",
   "metadata": {},
   "source": [
    "Note that for each logit, its partial derivative is negative for the correct label and is positive, but very small, for the incorrect labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f7a519b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9593,  0.0629,  0.0300,  0.0288,  0.0354,  0.0283,  0.0308,  0.0553,\n",
       "         0.0267,  0.0339,  0.0408,  0.0192,  0.0136,  0.0702,  0.0643,  0.0320,\n",
       "         0.0132,  0.0280,  0.0454,  0.0319,  0.0247,  0.0313,  0.0530,  0.0339,\n",
       "         0.0595,  0.0268,  0.0397], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084177ce",
   "metadata": {},
   "source": [
    "The sum across the partial derivative of the logits for each example is approximately $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "910e301d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7940e-09, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cffd7d0",
   "metadata": {},
   "source": [
    "So the gradient is kind of like a force for the logits, where the correct label pulls it in one direction, and the incorrect labels pull it slightly in the opposite direction, and the net force is approximately $0$. Since the neurons are all connected, eventually the force of pulling on these logits causes a pull on the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac320fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach().cpu(), cmap='gray')\n",
    "plt.title('Gradient of Logits')\n",
    "plt.xlabel('Character')\n",
    "plt.ylabel('Example in batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ce8a6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 1]),\n",
       " torch.Size([32, 27]),\n",
       " torch.Size([32, 27]),\n",
       " torch.Size([32, 27]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum.shape, logits.shape, logprobs.shape, probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797b8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(batch_size), Yb].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89a6296f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
